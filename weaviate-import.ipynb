{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import weaviate \n",
    "\n",
    "\n",
    "\n",
    "client = weaviate.connect_to_wcs(\n",
    "    cluster_url=\"https://dspy-cluster-8vxn8hp7.weaviate.network\",  # Replace with your WCS URL\n",
    "    auth_credentials=weaviate.auth.AuthApiKey(\"u4JlAXdnZnMcEfBUudwyG2kWW1AkinBunj6z\"),\n",
    "    skip_init_checks=True , # Replace with your WCS key\n",
    "    headers={\n",
    "        'X-Cohere-Api-Key': \"J5iCZDAETMccnIgjnjYhRkvp3DlcWUueEjkdkZSS\" # Replace with your Cohere API key\n",
    "    }\n",
    ")\n",
    "\n",
    "# import weaviate\n",
    "# import os\n",
    "  \n",
    "# # Set these environment variables\n",
    "# URL = os.getenv(\"WCS_URL\")\n",
    "# APIKEY = os.getenv(\"WCS_API_KEY\")\n",
    "  \n",
    "# # Connect to a WCS instance\n",
    "# client = weaviate.connect_to_wcs(\n",
    "#     cluster_url=URL,\n",
    "#     auth_credentials=weaviate.auth.AuthApiKey(APIKEY))\n",
    "\n",
    "# client = weaviate.Client(\n",
    "#   url=\"https://dspy-cluster-8vxn8hp7.weaviate.network\",\n",
    "#   auth_client_secret=weaviate.auth.AuthApiKey(\"u4JlAXdnZnMcEfBUudwyG2kWW1AkinBunj6z\"),\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnexpectedStatusCodeError",
     "evalue": "Collection may not have been created properly.! Unexpected status code: 422, with response body: {'error': [{'message': 'class name \"WeaviateBlogChunk\" already exists'}]}.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnexpectedStatusCodeError\u001b[0m                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mweaviate\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclasses\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mwvcc\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m collection \u001b[38;5;241m=\u001b[39mclient\u001b[38;5;241m.\u001b[39mcollections\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m      4\u001b[0m     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWeaviateBlogChunk\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      5\u001b[0m     vectorizer_config\u001b[38;5;241m=\u001b[39mwvcc\u001b[38;5;241m.\u001b[39mConfigure\u001b[38;5;241m.\u001b[39mVectorizer\u001b[38;5;241m.\u001b[39mtext2vec_cohere\n\u001b[1;32m      6\u001b[0m     (\n\u001b[1;32m      7\u001b[0m         model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membed-multilingual-v3.0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      8\u001b[0m     ),\n\u001b[1;32m      9\u001b[0m     properties\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m     10\u001b[0m             wvcc\u001b[38;5;241m.\u001b[39mProperty(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m, data_type\u001b[38;5;241m=\u001b[39mwvcc\u001b[38;5;241m.\u001b[39mDataType\u001b[38;5;241m.\u001b[39mTEXT),\n\u001b[1;32m     11\u001b[0m             wvcc\u001b[38;5;241m.\u001b[39mProperty(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauthor\u001b[39m\u001b[38;5;124m\"\u001b[39m, data_type\u001b[38;5;241m=\u001b[39mwvcc\u001b[38;5;241m.\u001b[39mDataType\u001b[38;5;241m.\u001b[39mTEXT),\n\u001b[1;32m     12\u001b[0m       ]\n\u001b[1;32m     13\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/llms/lib/python3.11/site-packages/weaviate/collections/collections.py:130\u001b[0m, in \u001b[0;36m_Collections.create\u001b[0;34m(self, name, description, generative_config, inverted_index_config, multi_tenancy_config, properties, references, replication_config, reranker_config, sharding_config, vector_index_config, vectorizer_config, data_model_properties, data_model_references, skip_argument_validation)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ValidationError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m WeaviateInvalidInputError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid collection config create parameters.\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m--> 130\u001b[0m name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m_create(config\u001b[38;5;241m.\u001b[39m_to_dict())\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m    132\u001b[0m     config\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m==\u001b[39m name\n\u001b[1;32m    133\u001b[0m ), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mName of created collection (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) does not match given name (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget(\n\u001b[1;32m    135\u001b[0m     name,\n\u001b[1;32m    136\u001b[0m     data_model_properties,\n\u001b[1;32m    137\u001b[0m     data_model_references,\n\u001b[1;32m    138\u001b[0m     skip_argument_validation\u001b[38;5;241m=\u001b[39mskip_argument_validation,\n\u001b[1;32m    139\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/llms/lib/python3.11/site-packages/weaviate/collections/base.py:59\u001b[0m, in \u001b[0;36m_CollectionsBase._create\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create\u001b[39m(\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     57\u001b[0m     config: \u001b[38;5;28mdict\u001b[39m,\n\u001b[1;32m     58\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m---> 59\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection\u001b[38;5;241m.\u001b[39mpost(\n\u001b[1;32m     60\u001b[0m         path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/schema\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     61\u001b[0m         weaviate_object\u001b[38;5;241m=\u001b[39mconfig,\n\u001b[1;32m     62\u001b[0m         error_msg\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCollection may not have been created properly.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     63\u001b[0m         status_codes\u001b[38;5;241m=\u001b[39m_ExpectedStatusCodes(ok_in\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m, error\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreate collection\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     64\u001b[0m     )\n\u001b[1;32m     66\u001b[0m     collection_name \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mjson()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(collection_name, \u001b[38;5;28mstr\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/llms/lib/python3.11/site-packages/weaviate/connect/v4.py:480\u001b[0m, in \u001b[0;36m_Connection.post\u001b[0;34m(self, path, weaviate_object, params, error_msg, status_codes)\u001b[0m\n\u001b[1;32m    472\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m    473\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    474\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    478\u001b[0m     status_codes: Optional[_ExpectedStatusCodes] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    479\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Response:\n\u001b[0;32m--> 480\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__send(\n\u001b[1;32m    481\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPOST\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    482\u001b[0m         url\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murl \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_api_version_path \u001b[38;5;241m+\u001b[39m path,\n\u001b[1;32m    483\u001b[0m         weaviate_object\u001b[38;5;241m=\u001b[39mweaviate_object,\n\u001b[1;32m    484\u001b[0m         params\u001b[38;5;241m=\u001b[39mparams,\n\u001b[1;32m    485\u001b[0m         error_msg\u001b[38;5;241m=\u001b[39merror_msg,\n\u001b[1;32m    486\u001b[0m         status_codes\u001b[38;5;241m=\u001b[39mstatus_codes,\n\u001b[1;32m    487\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/llms/lib/python3.11/site-packages/weaviate/connect/v4.py:431\u001b[0m, in \u001b[0;36m_Connection.__send\u001b[0;34m(self, method, url, error_msg, status_codes, weaviate_object, params)\u001b[0m\n\u001b[1;32m    429\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39msend(req)\n\u001b[1;32m    430\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status_codes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m res\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m status_codes\u001b[38;5;241m.\u001b[39mok:\n\u001b[0;32m--> 431\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m UnexpectedStatusCodeError(error_msg, response\u001b[38;5;241m=\u001b[39mres)\n\u001b[1;32m    432\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(Response, res)\n\u001b[1;32m    433\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mUnexpectedStatusCodeError\u001b[0m: Collection may not have been created properly.! Unexpected status code: 422, with response body: {'error': [{'message': 'class name \"WeaviateBlogChunk\" already exists'}]}."
     ]
    }
   ],
   "source": [
    "import weaviate.classes.config as wvcc\n",
    "\n",
    "collection =client.collections.create(\n",
    "    name=\"WeaviateBlogChunk\",\n",
    "    vectorizer_config=wvcc.Configure.Vectorizer.text2vec_cohere\n",
    "    (\n",
    "        model=\"embed-multilingual-v3.0\"\n",
    "    ),\n",
    "    properties=[\n",
    "            wvcc.Property(name=\"content\", data_type=wvcc.DataType.TEXT),\n",
    "            wvcc.Property(name=\"author\", data_type=wvcc.DataType.TEXT),\n",
    "      ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "def chunk_list(lst, chunk_size):\n",
    "    \"\"\"Break a list into chunks of the specified size.\"\"\"\n",
    "    return [lst[i:i + chunk_size] for i in range(0, len(lst), chunk_size)]\n",
    "\n",
    "def split_into_sentences(text):\n",
    "    \"\"\"Split text into sentences using regular expressions.\"\"\"\n",
    "    sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', text)\n",
    "    return [sentence.strip() for sentence in sentences if sentence.strip()]\n",
    "\n",
    "def read_and_chunk_index_files(main_folder_path):\n",
    "    \"\"\"Read index.md files from subfolders, split into sentences, and chunk every 5 sentences.\"\"\"\n",
    "    blog_chunks = []\n",
    "    for folder_name in os.listdir(main_folder_path):\n",
    "        subfolder_path = os.path.join(main_folder_path, folder_name)\n",
    "        if os.path.isdir(subfolder_path):\n",
    "            index_file_path = os.path.join(subfolder_path, 'index.mdx')\n",
    "            if os.path.isfile(index_file_path):\n",
    "                with open(index_file_path, 'r', encoding='utf-8') as file:\n",
    "                    content = file.read()\n",
    "                    sentences = split_into_sentences(content)\n",
    "                    sentence_chunks = chunk_list(sentences, 5)\n",
    "                    sentence_chunks = [' '.join(chunk) for chunk in sentence_chunks]\n",
    "                    blog_chunks.extend(sentence_chunks)\n",
    "    return blog_chunks\n",
    "\n",
    "# Example usage\n",
    "main_folder_path = './blog'\n",
    "blog_chunks = read_and_chunk_index_files(main_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1182"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(blog_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'blog_chunks' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(blog_chunks[\u001b[38;5;241m0\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'blog_chunks' is not defined"
     ]
    }
   ],
   "source": [
    "print(blog_chunks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'blog_chunks' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01muuid\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m uuid4\n\u001b[1;32m      4\u001b[0m blogs \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mcollections\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWeaviateBlogChunk\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, blog_chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(blog_chunks):\n\u001b[1;32m      7\u001b[0m     upload \u001b[38;5;241m=\u001b[39m blogs\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m      8\u001b[0m         properties\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m      9\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: blog_chunk\n\u001b[1;32m     10\u001b[0m         }\n\u001b[1;32m     11\u001b[0m     )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'blog_chunks' is not defined"
     ]
    }
   ],
   "source": [
    "from weaviate.util import get_valid_uuid\n",
    "from uuid import uuid4\n",
    "\n",
    "blogs = client.collections.get(\"WeaviateBlogChunk\")\n",
    "\n",
    "for idx, blog_chunk in enumerate(blog_chunks):\n",
    "    upload = blogs.data.insert(\n",
    "        properties={\n",
    "            \"content\": blog_chunk\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<weaviate.collections.cluster._Cluster at 0x1130b1d10>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "client.schema.get()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "client.collections.get(\"WeaviateBlogChunk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "first = client.collections.get(\"WeaviateBlogChunk\")\n",
    "response = first.query.fetch_objects()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now, you can expose Prometheus-compatible metrics for monitoring. Combine this with a standard Prometheus/Grafana setup to create visual dashboards for metrics around latencies, import speed, time spent on vector vs object storage, memory usage, and more. ![Importing Data into Weaviate](./img/weaviate-sample-dashboard-importing.png \"Importing Data Into Weaviate\")\n",
      "\n",
      "### Example\n",
      "In a hypothetical scenario, you might be importing a large dataset. At one point the import process might slow down. You could then check your dashboards, where you might see that the vector indexing process is still running fast, while the object indexing slowed down.\n",
      "He developed an Autonomous Testing Agent to enhance software testing efficiency, harnessing the power of [SuperAGI](https://www.linkedin.com/company/superagi/) and Weaviate. Meanwhile, [BYTE](https://lablab.ai/event/cohere-coral-hackathon/byte/byte-ai-based-nutrition-app), an AI-based nutrition app, clinched the top spot at the [Coral Cohere Hackathon](https://lablab.ai/event/cohere-coral-hackathon)! Ayesha and Moneebah built this project to transform and personalize nutrition advice. They used Weaviate’s vector database for search and recommendation and multi-tenancy for data security. These projects offer just a glimpse of the boundless possibilities within the AI realm, pointing the way to a future where AI is more accessible, formidable, and transformative. So, what are you waiting for if you haven't already started building with Weaviate?\n",
      "<!-- TODO: update with a link to the article once it is ready -->\n",
      "*We are working on an article that will guide you on how to create your own model and upload it to Hugging Face.*\n",
      "\n",
      "### Fully automated and optimized\n",
      "Weaviate manages the whole process for you. From the perspective of writing your code – once you have your schema configuration – you can almost forget that Hugging Face is involved at all. For example, when you import data into Weaviate, Weaviate will automatically extract the relevant text fields, send them Hugging Face to vectorize, and store the data with the new vectors in the database. ### Ready to use with a minimum of fuss\n",
      "Every new Weaviate instance created with the [Weaviate Cloud Services](/pricing) has the Hugging Face module enabled out of the box. You don't need to update any configs or anything, it is there ready and waiting.\n",
      "Fortunately, there are emerging technologies that help solve this limitation. <!-- truncate -->\n",
      "\n",
      "[LangChain](https://langchain.readthedocs.io/en/latest/) is one of the most exciting new tools in AI. LangChain helps overcome many limitations of LLMs such as hallucination and limited input lengths. Hallucination refers to where the LLM generates a response that is not supported by the input or context – meaning it will output text that is irrelevant, inconsistent, or misleading. As you can imagine, this is a huge problem in many applications.\n",
      "#### Solution\n",
      "We addressed each of the points above individually and improved the overall MTTR substantially:\n",
      "\n",
      "- A deduplication process was added, so that large WALs with a lot of updates (i.e. redundant data) could be reduced to only the necessary information. - The recovery process now runs in parallel. If there are multiple places that require recovery, they can each recover independently, without one recovery having to wait for the other. - A mechanism was added that flushes any memtable that has been idle (no writes) for 60s or more. In addition to speeding up the recovery, this change also ensures that no recovery is needed at all in many cases.\n",
      "---\n",
      "title: Support for Hugging Face Inference API in Weaviate\n",
      "slug: hugging-face-inference-api-in-weaviate\n",
      "authors: [sebastian]\n",
      "date: 2022-09-27\n",
      "tags: ['integrations']\n",
      "image: ./img/hero.png\n",
      "description: \"Running ML Model Inference in production is hard. You can use Weaviate – a vector database – with Hugging Face Inference module to delegate the heavy lifting.\"\n",
      "---\n",
      "![Support for Hugging Face Inference API in Weaviate](./img/hero.png)\n",
      "\n",
      "<!-- truncate -->\n",
      "\n",
      "Vector databases use Machine Learning models to offer incredible functionality to operate on your data. We are looking at anything from **summarizers** (that can summarize any text into a short) sentence), through **auto-labelers** (that can classify your data tokens), to **transformers** and **vectorizers** (that can convert any data – text, image, audio, etc. – into vectors and use that for context-based queries) and many more use cases. All of these use cases require `Machine Learning model inference` – a process of running data through an ML model and calculating an output (e.g. take a paragraph, and summarize into to a short sentence) – which is a compute-heavy process.\n",
      "We would be amiss to not mention other search methods such as BM25F, or hybrid searches, both of which would be affected by these decisions. Now that you've seen exactly what happens behind the curtains, we encourage you to try applying these concepts yourself the next time you are building something with Weaviate. While the changes to the similarities were somewhat minor in our examples, in some domains and corpora their impact may be certainly larger. And tweaking the exact vectorization scheme may provide that extra boost your Weaviate instance is looking for. import StayConnected from '/_includes/stay-connected.mdx'\n",
      "\n",
      "<StayConnected />\n",
      "A larger ef results in more distance comparisons done during the search, slowing it down significantly although producing a more accurate result. The next parameters to look at are the ones used in index building, efConstruction, the size of the queue when inserting data into the graph, and maxConnections, the number of edges per node, which also must be stored with each vector. Another new direction we are exploring is the impact of distribution shift on PQ centroids and the intersection with hybrid clustering and graph index algorithms such as [DiskANN](https://suhasjs.github.io/files/diskann_neurips19.pdf) or [IVFOADC+G+P](https://openaccess.thecvf.com/content_ECCV_2018/papers/Dmitry_Baranchuk_Revisiting_the_Inverted_ECCV_2018_paper.pdf). Using the Recall metric may be a good enough measure of this to trigger re-fitting the centroids, with the question then being: which subset of vectors to use in re-fitting. If we use the last 100K that may have caused the recall drop, we could risk overfitting to the new distribution, thus we likely want some hybrid sampling of the timeline of our data distribution when inserted into Weaviate.\n",
      "LLMs have a limited input length when referring to the scale of inputting a book or pages of search results. LangChain has various techniques implemented to solve this problem. This blog post will begin by explaining some of the key concepts introduced in LangChain and end with a demo. The demo will show you how to combine LangChain and Weaviate to build a custom LLM chatbot powered with semantic search!\n",
      "\n",
      "## Sequential Chains\n",
      "[Chains](https://python.langchain.com/docs/modules/chains/) enable us to combine multiple LLM inferences together. As you can guess from the name, sequential chains execute their links in a sequential order.\n",
      "In conclusion, efforts on developing multimodal models attempt to mimic human learning by combining different inputs, such as images, text, and audio, to improve the performance and robustness of machine learning systems. By leveraging multi-sensory inputs, these models can learn to recognize complex multimodal patterns, understand context across modes, and generate more comprehensive and accurate outputs even in the absence of some modalities. The main goal is to give these models the ability to interact with data in a more natural way thus enabling them to be more powerful and general reasoning engines. ## Multimodal Models in Weaviate\n",
      "\n",
      "Currently, the only out-of-the-box multimodal module that can be configured and used with Weaviate is `multi2vec-clip` which can be used to project images and text into a joint embedding space and then perform a `nearVector` or `nearImage` search over these two modalities. Outside of this, you can only use multimodal models if they are hosted on Huggingface or if you have your own proprietary multimodal models.\n",
      "Within a schema, you can set different vectorizers and vectorize instructions on a class level. First, because our use case is semantic search over Wikipedia, we will be dividing the dataset into paragraphs and use Weaviate's graph schema to link them back to the articles. Therefore we need two classes; *Article* and *Paragraph*. ```javascript\n",
      "{\n",
      "  classes: [\n",
      "    {\n",
      "      class: \"Article\",\n",
      "      description: \"A wikipedia article with a title\",\n",
      "      properties: {...},\n",
      "      vectorIndexType: \"hnsw\",\n",
      "      vectorizer: \"none\"\n",
      "    },\n",
      "    {\n",
      "      class: \"Paragraph\",\n",
      "      description: \"A wiki paragraph\",\n",
      "      properties: {...},\n",
      "      vectorIndexType: \"hnsw\",\n",
      "      vectorizer: \"text2vec-transformers\"\n",
      "    },\n",
      "  ]\n",
      "}\n",
      "```\n",
      "\n",
      "*Weaviate class structure*\n",
      "\n",
      "Next, we want to make sure that the content of the paragraphs gets vectorized properly, the vector representations that the SentenceBERT transformers will generate are used for all our semantic search queries. ```javascript\n",
      "{\n",
      "  name: \"content\",\n",
      "  datatype: [\n",
      "    \"text\"\n",
      "  ],\n",
      "  description: \"The content of the paragraph\",\n",
      "  invertedIndex: false,\n",
      "  moduleConfig: {\n",
      "    text2vec-transformers: {\n",
      "      skip: false,\n",
      "      vectorizePropertyName: false\n",
      "    }\n",
      "  }\n",
      "}\n",
      "```\n",
      "\n",
      "*A single data type that gets vectorized*\n",
      "\n",
      "Last, we want to make graph relations, in the dataset from step one we will distill all the graph relations between articles that we can reference like this:\n",
      "\n",
      "```javascript\n",
      "{\n",
      "  name: \"hasParagraphs\"\n",
      "  dataType: [\n",
      "    \"Paragraph\"\n",
      "  ],\n",
      "  description: \"List of paragraphs this article has\",\n",
      "  invertedIndex: true\n",
      "}\n",
      "```\n",
      "*Paragraph cross-references*\n",
      "\n",
      "The complete schema we import using the [Python client](/developers/weaviate/client-libraries/python) can be found [here](https://github.com/weaviate/semantic-search-through-wikipedia-with-weaviate/blob/main/step-2/import.py#L19-L120).\n",
      "We were able to import 200 million objects and more, while the import performance remained constant throughout the process. [See more on github](https://github.com/weaviate/weaviate/pull/1976). ### Drastically improved Mean-Time-To-Recovery (MTTR)\n",
      "Weaviate `1.14` fixes an issue where a crash-recovery could take multiple minutes, or even hours in some extreme cases. It is now a matter of just seconds. So even in the rare event that your instance crashes, it will come back up very quickly.\n",
      "Let's explore how PQ works. ## Product Quantization\n",
      "![ann](./img/Ann.png)\n",
      "\n",
      "If you already know the details behind how Product Quantization works feel free to skip this section!\n",
      "\n",
      "The main intuition behind Product Quantization is that it adds the concept of segments to the compression function. Basically, to compress the full vector, we will chop it up and operate on segments of it. For example, if we have a 128 dimensional vector we could chop it up into 32 segments, meaning each segment would contain 4 dimensions. Following this segmentation step we compress each segment independently.\n",
      "Word2vec in particular uses a neural network [model](https://arxiv.org/pdf/1301.3781.pdf) to learn word associations from a large corpus of text (it was initially trained by Google with 100 billion words). It first creates a vocabulary from the corpus, then learns vector representations for the words, typically with 300 dimensions. Words found in similar contexts have vector representations that are close in vector space, but each word from the vocabulary has only one resulting word vector. Thus, the meaning of words can be quantified - “run” and “ran” are recognized as being far more similar than “run” and “coffee”, but words like “run” with multiple meanings have only one vector representation. As the name suggests, word2vec is a word-level model and cannot by itself produce a vector to represent longer text such as sentences, paragraphs or documents.\n",
      "And instead of the `text2vec-cohere` module, we will go straight to the Cohere API. Concatenating the text from the object:\n",
      "\n",
      "```python\n",
      "str_in = ' '.join([i for i in properties.values()])\n",
      "```\n",
      "\n",
      "We see:\n",
      "\n",
      "```text\n",
      "'In 1963, live on \"The Art Linkletter Show\", this company served its billionth burger McDonald\\'s'\n",
      "```\n",
      "\n",
      "Then, use the Cohere API to generate the vector like so, where `cohere_key` is the API key (keep it secret!), and `model` is the vectorizer. ```python\n",
      "import cohere\n",
      "co = cohere.Client(cohere_key)\n",
      "co_resp = co.embed([str_in], model=\"embed-multilingual-v2.0\")\n",
      "```\n",
      "\n",
      "Then we run a `nearVector` based query to find the best matching object to this vector:\n",
      "\n",
      "```python\n",
      "client.query.get(\n",
      "    \"Question\",\n",
      "    [\"question\", \"answer\"]\n",
      ").with_limit(2).with_near_vector(\n",
      "    {'vector': co_resp.embeddings[0]}\n",
      ").with_additional(['distance']).do()\n",
      "```\n",
      "\n",
      "Interestingly, we get a distance of `0.0181` - small, but not zero. In other words - we did something differently to Weaviate!\n",
      "\n",
      "Let's go through the differences one by one, which hopefully will help you to see Weaviate's default behavior. First, Weaviate sorts properties alphabetically (a-z) before concatenation.\n",
      "How, you ask? Having redundancy is key to achieving this. And such redundancy, or replication, has been available for a while in Weaviate for production use. In fact, configuring replication is actually easy and simple, and using it can lead to huge benefits. In this post, we will explain the benefits of replication and show you how to configure it.\n",
      "This is also known as weak supervision in machine learning. Once a dataset has been prepared, there are three common metrics used for evaluation: **nDCG**, **Recall**, and **Precision**. NDCG (Normalized Discounted Cumulative Gain) measures ranking with multiple relevance labels. For example, a document about Vitamin B12 may not be the most relevant result to a query about Vitamin D, but it is more relevant than a document about the Boston Celtics. Due to the additional difficulty of relative ranking, binary relevance labels are often used (1 for relevant, 0 for irrelevant).\n",
      "This includes [improved APIs](https://github.com/weaviate/weaviate-python-client/issues/205) on the client side, new modules, for example, for [generative search](/developers/weaviate/modules/reader-generator-modules/generative-openai), and improvements to our existing modules. <br></br>\n",
      "\n",
      "### Community\n",
      "![community](./img/community.png)\n",
      "\n",
      "The most important pillar is all of you – our community. This includes both free, open-source users that self-host their Weaviate setup, as well as paid enterprise users and anyone using our Weaviate-as-a-Service offerings. We value your feedback and love that you are part of shaping our future. Last year we introduced our [dynamic roadmap page](/developers/weaviate/roadmap) that allows you to create and upvote your favorite feature requests.\n",
      "---\n",
      "title: Vector Embeddings Explained\n",
      "slug: vector-embeddings-explained\n",
      "authors: [dan]\n",
      "date: 2023-01-16\n",
      "tags: ['concepts']\n",
      "image: ./img/hero.png\n",
      "description: \"Get an intuitive understanding of what exactly vector embeddings are, how they're generated, and how they're used in semantic search.\"\n",
      "---\n",
      "The core function of Weaviate is to provide high-quality search results, going beyond simple keyword or synonym searches, and actually finding what the user _means_ by the query, or providing an actual answer to questions the user asks. <!-- truncate -->\n",
      "\n",
      "Semantic searches (as well as question answering) are essentially searches by similarity, such as by the meaning of text, or by what objects are contained in images. For example, consider a library of wine names and descriptions, one of which mentioning that the wine is “good with **fish**”. A “wine for **seafood**” keyword search, or even a synonym search, won’t find that wine. A meaning-based search should understand that “fish” is similar to “seafood”, and “good with X” means the wine is “for X”—and should find the wine.\n",
      "### Announcement\n",
      "\n",
      "🎉We are happy to share that all Weaviate `v1.15` binaries and distributions have been **compiled with Go 1.19** which comes with **GOMEMLIMIT**. Now, you can set your **soft memory cap** by setting the `GOMEMLIMIT` environment variable like this:\n",
      "\n",
      "```\n",
      "GOMEMLIMIT=120GiB\n",
      "```\n",
      "\n",
      "For more information, see the [Docker Compose environment variables](/developers/weaviate/installation/docker-compose#environment-variables) in the docs. ## Faster imports for ordered data\n",
      "\n",
      "![Faster imports for ordered data](./img/ordered-imports.png)\n",
      "\n",
      "Weaviate `v1.5` introduced an **LSM store** (Log-Structured Merge Trees) to increase write-throughput. The high-level idea is that writes are batched up in logs, sorted into a **Binary Search Tree** (BST) structure, and then these batched-up trees are merged into the tree on disk. ### The Problem\n",
      "\n",
      "When importing objects with an inherent order, such as timestamps or row numbers that increase monotonously, the BST becomes unbalanced: New objects are always inserted at the \"greater than\" pointer / right node of the BST.\n",
      "#### Test\n",
      "We designed an extreme stress test that would represent the \"worst-case\" scenario for recovery. It has multiple very large, independent Write-Ahead Logs that required for recovery. Before, this could take many hours to recover, while now it takes only a few seconds. ### Full changelog\n",
      "These are few of the many improvements and bug fixes that were included in this release. Check out [the changelog](https://github.com/weaviate/weaviate/releases/tag/v1.14.0) to see the complete list.\n",
      "## How are vector embeddings generated? The magic of vector search resides primarily in how the embeddings are generated for each entity and the query, and secondarily in how to efficiently search within very large datasets (see our [“Why is Vector Search so Fast”](/blog/why-is-vector-search-so-fast) article for the latter). As we mentioned, vector embeddings can be generated for various media types such as text, images, audio and others. For text, vectorization techniques have evolved tremendously over the last decade, from the venerable [word2vec](https://en.wikipedia.org/wiki/Word2vec) ([2013](https://code.google.com/archive/p/word2vec/)), to the state-of-the-art transformer models era, spurred by the release of [BERT](https://en.wikipedia.org/wiki/BERT_(language_model)) in [2018](https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html). ### Word-level dense vector models (word2vec, GloVe, etc.)\n",
      "[word2vec](https://wiki.pathmind.com/word2vec) is a [family of model architectures](https://www.tensorflow.org/tutorials/text/word2vec) that introduced the idea of “dense” vectors in language processing, in which all values are non-zero.\n",
      "Some models, such as [CLIP](https://openai.com/blog/clip/), are capable of vectorizing multiple data types (images and text in this case) into one vector space, so that an image can be searched by its content using only text. ## Vector embeddings with Weaviate\n",
      "\n",
      "For this reason, Weaviate is configured to support many different vectorizer models and vectorizer service providers. You can even [bring your own vectors](/developers/weaviate/starter-guides/custom-vectors), for example if you already have a vectorization pipeline available, or if none of the publicly available models are suitable for you. For one, Weaviate supports using any Hugging Face models through our `text2vec-hugginface` module, so that you can [choose one of the many sentence transformers published on Hugging Face](/blog/how-to-choose-a-sentence-transformer-from-hugging-face). Or, you can use other very popular vectorization APIs such as OpenAI or Cohere through the [`text2vec-openai`](/developers/weaviate/modules/retriever-vectorizer-modules/text2vec-openai) or [`text2vec-cohere`](/developers/weaviate/modules/retriever-vectorizer-modules/text2vec-cohere) modules.\n",
      "Besides access isolation, ACME Accounting has other requirements for a multi-tenancy setup:\n",
      "\n",
      "* **Speed**: With millions of tenants, narrowing a request down to a single tenant should not take much work. * **Easy on and offboarding**: Just because ACME Accounting already has a million customers doesn’t mean that adding your business to ACME should take a considerable computational load. If AliceCorp cancels its contract, this should not affect other tenants. * **Resource boundaries**: If members of BobInc all get together to produce their annual report, this can put a lot of load onto ACME’s system. This should not interfere with AliceCorp, which might also have essential accounting deadlines.\n",
      "Your answer to the question must be grounded in the provided search results and nothing else!!”. As described earlier, Few-Shot Examples describes collecting a few manually written examples of question, context, answer pairs to guide the language model’s generation. Recent research such as [“In-Context Vectors”](https://arxiv.org/abs/2311.06668) are further pointing to the importance of guiding latent space like this. We were using GPT-3.5-turbo to generate Weaviate queries in the Weaviate Gorilla project and performance skyrocketed once we added few-shot examples of natural language to query translations. Lastly, there is increasing interest in fine-tuning LLMs for RAG applications.\n",
      "* [**RAG Metrics**](#rag-metrics): Common metrics used to evaluate Generation, Search, and Indexing and how they interact with each other. * [**RAG: Knobs to Tune**](#rag-knobs-to-tune): What decision makes RAG systems perform significantly different from one another? * [**Orchestrating Tuning**](#tuning-orchestration): How do we manage tracking experimental configurations of RAG systems? * [**From RAG to Agent Evaluation**](#from-rag-to-agent-evaluation): We define RAG as a three step procss of index, retrieve, and generate. This section describes when a RAG system becomes an Agent system and how Agent Evaluation differs.\n",
      "Just add one of the following properties to the `POST payload`:\n",
      "* `include` - an array class names we want to backup or restore\n",
      "* `exclude` - an array class names we don't want to backup or restore\n",
      "\n",
      "For example, you can create a backup that includes Cats, Dogs and Meerkats. ```js\n",
      "POST /v1/backups/gcs\n",
      "{\n",
      "  \"id\": \"first_backup\",\n",
      "  \"include\": [\"Cats\", \"Dogs\", \"Meerkats\"]\n",
      "}\n",
      "```\n",
      "\n",
      "Then restore all classes, excluding Cats:\n",
      "\n",
      "```js\n",
      "POST /v1/backups/gcs/first_backup/restore\n",
      "{\n",
      "  \"exclude\": [\"Cats\"]\n",
      "}\n",
      "```\n",
      "\n",
      "### Other use cases\n",
      "It might not be immediately obvious, but you can use the above workflow to migrate your data to other environments. So, if one day you find yourself with an environment that is not set up for what you need (i.e. not enough resources). Then create a backup, and restore it in the new environment. 😉\n",
      "\n",
      "### Follow up\n",
      "Are you ready to set up backups for your environment?\n",
      "See below for the Mona Lisa drip painted in the style of Jackson Pollock!\n",
      "\n",
      "![Mona lisa drip painting](./img/the_mona_lisa_drip_painted.jpg)\n",
      "\n",
      "The technology behind these images is called **diffusion models**. In this post I aim to provide a gentle introduction to diffusion models so that even someone with minimal understanding of machine learning or the underlying statistical algorithms will be able to build a general intuition of how they work. Additionally I’ll also provide some external resources that you can use to access pre-trained diffusion models so you can start to generate your own art!\n",
      "\n",
      "These are the points that we’ll expand on in this article:\n",
      "\n",
      "- How diffusion models can create realistic images\n",
      "- How and why we can control and influence the images these models create using text prompts\n",
      "- Access to some resources you can use to generate your own images. ## How Diffusion Models Work\n",
      "\n",
      "Diffusion models are a type of generative model - which means that they can generate data points that are similar to the data points they’ve been trained on(the training set). So when we ask Stable Diffusion to create an image it starts to dream up images that are similar to the billions of images from the internet that it was trained on - it’s important to note that it doesn’t simply copy an image from the training set(which would be no fun!) but rather creates a new image that is similar to the training set.\n",
      "3**: *We are compressing a 128 dimensions vector into a 32 bytes compressed vector. For this, we define 32 segments meaning the first segment is composed of the first four dimensions, the second segment goes from dimension 5th to 8th and so on. Then for each segment we need a compression function that takes a four dimensional vector as an input and returns a byte representing the index of the center which best matches the input. The decompression function is straightforward, given a byte, we reconstruct the segment by returning the center at the index encoded by the input.*\n",
      "\n",
      "A straightforward encoding/compression function uses KMeans to generate the centers, each of which can be represented using an id/code and then each incoming vector segment can be assigned the id/code for the center closest to it. Putting all of this together, the final algorithm would work as follows: Given a set of N vectors, we segment each of them producing smaller dimensional vectors, then apply KMeans clustering per segment over the complete data and find 256 centroids that will be used as predefined centers.\n",
      "There are many different ANN algorithms, each with different advantages and limitations. ### Large Scale\n",
      "When we talk about a vast number of objects, today we often see use cases with hundreds of millions of vectors, but it won't take long until billions, or even trillions, will be a pretty standard use case. To get vector databases to that kind of scale, we need to constantly evolve and look for more efficient solutions. A big part of this search is to explore ANN algorithms that would let us go beyond the available RAM (which is a bit of a bottleneck) without sacrificing the performance and the UX. ### What to expect\n",
      "In this series of blog posts, we will take you on a journey with us as we research and implement new ANN algorithms in our quest to reach the 1T goal.\n",
      "Prompt tuning, 2. Few-Shot Examples, and 3. Fine-Tuning. Prompt tuning entails tweaking the particular language used such as: “Please answer the question based on the provided search results.” versus “Please answer the question. IMPORTANT, please follow these instructions closely.\n",
      ">\n",
      "> For GCS you can use a Google Application Credentials json file. Alternatively, you can configure backups with the **local filesystem**. All you need here is to provide the path to the backup folder. > Note, you can have multiple storage configurations - one for each S3, GCS and the local filesystem. ### Creating backups - API\n",
      "Once you have the backup module up and running, you can create backups with a single `POST` command:\n",
      "\n",
      "```js\n",
      "POST /v1/backups/{storage}/\n",
      "{\n",
      "  \"id\": \"backup_id\"\n",
      "}\n",
      "```\n",
      "\n",
      "The `storage` values are `s3`, `gcs`, and `filesystem`.\n",
      "Thus you could take any image from your training set, and step by step, add increasing levels of random noise to it and generate incrementally more noisy versions of that image as shown below. ![noising gif](./img/noise.gif)\n",
      "*[Source](https://yang-song.net/blog/2021/score/)*\n",
      "\n",
      "![noising images](./img/noisingimage.png)\n",
      "*[Source](https://huggingface.co/blog/annotated-diffusion)*\n",
      "\n",
      "This “noising” process, shown in the images above allows us to take training set images and add known quantities of noise to it until it becomes completely random noise. This process takes images from a state of having high probability of being found in the training set to having a low probability of existing in the training set. Once the “noising” step is completed, then we can use these clean and noisy image combinations during the training phase of the diffusion model. In order to train a diffusion model we ask it to remove the noise from the noised images step by step until it recovers something as close as possible to the original image.\n",
      "The embeddings are placed into an index, so that the database can [quickly](/blog/why-is-vector-search-so-fast) perform searches. 3. For each query,\n",
      "    1. a vector embedding is computed using the same model that was used for the data objects. 2.\n",
      "Almost all objects are unique and it is not a problem to process those concurrently. We found that what we really needed was just a lock for each unique UUID. Cleverly, this approach would ensure that only one object per UUID is handled at each point in time, so that Weaviate cannot add multiple instances of objects with the same UUID. Meanwhile, it would still allow full parallelization of import processes to maximize performance. ![Single-lock solution](./img/single-lock-solution.png)\n",
      "\n",
      "As it often happens, implementing a lock-per-key solution created a different issue.\n",
      "---\n",
      "title: Weaviate 1.18 release\n",
      "slug: weaviate-1-18-release\n",
      "authors: [jp, erika, zain, dan]\n",
      "date: 2023-03-07\n",
      "tags: ['release']\n",
      "image: ./img/hero.png\n",
      "description: \"Weaviate 1.18 introduces Faster Filtering through Bitmap Indexing, HNSW-PQ, Cursor API, and more! Learn all about it.\"\n",
      "---\n",
      "\n",
      "import Core118 from './_core-1-18-include.mdx' ;\n",
      "\n",
      "<Core118 />\n",
      "\n",
      "import WhatsNext from '/_includes/what-next.mdx'\n",
      "\n",
      "<WhatsNext />\n",
      "\n",
      "import Ending from '/_includes/blog-end-oss-comment.md' ;\n",
      "\n",
      "<Ending />\n",
      "## RAG Metrics\n",
      "We are presenting RAG metrics from a top-down view from generation, to retrieval, and then indexing. We then present the RAG knobs to tune from a bottom-up perspective of building an index, tuning how to retrieve, and then options for generation. Another reason to present RAG Metrics from a top-down view is because errors from Indexing will bubble up to Search and then Generation, but errors in Generation (as we have defined the stack) have no impact on errors in Indexing. In the current state of RAG evaluation, it is uncommon to evaluate the RAG stack end-to-end, rather **oracle context**, or **controlled distractors** (such as the Lost in the Middle experiments) are assumed when determining faithfulness and answer relevancy in generation. Similarly, embeddings are typically evaluated with brute force indexing that doesn’t account for approximate nearest neighbor errors.\n",
      "### Endpoint changes\n",
      "This issue is now fixed with a **change to the API endpoints**. To get, modify and delete a data object, you now need to provide both the ID and the class name. The following object functions are changed: **GET**, **HEAD**, **PUT**, **PATCH** and **DELETE**. #### Object change\n",
      "New\n",
      "```\n",
      "/v1/objects/{ClassName}/{id}\n",
      "```\n",
      "Deprecated\n",
      "```\n",
      "/v1/objects/{id}\n",
      "```\n",
      "\n",
      "#### References change\n",
      "New\n",
      "```\n",
      "v1/objects/{ClassName}/{id}/references/{propertyName}\n",
      "```\n",
      "Deprecated\n",
      "```\n",
      "v1/objects/{id}/references/{propertyName}\n",
      "```\n",
      "\n",
      "### Client changes\n",
      "There are also updates in the language clients, where you now should provide a class name for data object manipulation. Old functions will continue to work, but are considered deprecated, and you will see a deprecation warning message.\n",
      "The PQ centroids fit with the first K vectors that enter Weaviate may be impacted by a significant shift in the data distribution. The continual training of machine learning models has a notorious “catastrophic forgetting” problem where training on the newest batch of data harms performance on earlier batches of data. This is also something that we are considering with the design of re-fitting PQ centroids. ## From RAG to Agent Evaluation\n",
      "Throughout the article, we have been concerned with **RAG**, rather than **Agent** Evaluation. In our view **RAG** is defined by the flow of Index, Retrieve, and Generate, whereas **Agents** have a more open-ended scope.\n",
      "---\n",
      "title: Multimodal Embedding Models\n",
      "slug: multimodal-models\n",
      "authors: zain\n",
      "date: 2023-06-27\n",
      "image: ./img/hero.png\n",
      "tags: ['concepts']\n",
      "description: \"ML Models that can see, read, hear and more!\"\n",
      "\n",
      "---\n",
      "\n",
      "![Multimodal Models](./img/hero.png)\n",
      "\n",
      "<!-- truncate -->\n",
      "\n",
      "## The Multisensory Nature of Human Learning\n",
      "\n",
      "Humans have a remarkable ability to learn and build world models through the integration of multiple sensory inputs. Our combination of senses work synergistically to provide us with rich and diverse information about our environment. By combining and interpreting these sensory inputs, we are able to form a coherent understanding of the world, make predictions, and acquire new knowledge very efficiently. The process of learning through multi-sensory inputs begins from the early stages of human development. Infants explore the world through their senses, touching, tasting, listening, and observing objects and people around them.\n",
      "For example, many of our users already run Weaviate with multi-tenancy (introduced in version `1.20`) to host thousands of active tenants or even more. One side effect of scaling is that as load increases on each node, it will take longer to start up. While a fresh Weaviate instance typically starts up essentially instantaneously, a node with 1000s of tenants can take up over 1 minute. Node-level downtime is an unavoidable fact of life, since either hardware and software may necessitate restarts for maintenance and/or updates. But node-level downtime doesn’t have to lead to user-level downtime and failed requests.\n",
      ":::\n",
      "\n",
      "## Conclusions\n",
      "We've managed to implement the indexing algorithm on DiskANN, and the resulting performance is good. From years of research & development, Weaviate has a highly optimized implementation of the HNSW algorithm. With the Vamana implementation, we achieved comparable in-memory results. There are still some challenges to overcome and questions to answer. For example:\n",
      "* How do we proceed to the natural disk solution of Weaviate?\n",
      "![Conceptual diagram of sending a request with authentication credentials](./img/auth_light.png#gh-light-mode-only)\n",
      "![Conceptual diagram of sending a request with authentication credentials](./img/auth_dark.png#gh-dark-mode-only)\n",
      "\n",
      "In other words, the server can provide as much access as the particular user is allowed. But balancing security with usability can be a tricky line to draw, as everybody has different needs and often use different systems. So, we thought that this might be a good time to provide an overview of all things authentication in Weaviate. Also, we've recently introduced an API key-based authentication method, which we think might be a good balance of security and usability for many of you. Please check them out below.\n",
      "This means that holding the vectors in memory requires 1,000,000 x 128 x 4 bytes = 512,000,000 bytes. Additionally, a graph representation of neighborhoods is built when indexing. The graph represents the k-nearest neighbors for each vector. To identify each neighbor we use an `int64`, meaning we need 8 bytes to store each of the k-nearest neighbors per vector. The parameter controlling the size of the graph is `maxConnections`.\n",
      "3**: *Summary of the previously presented results. We show results for the shortest and largest parameter sets for uncompressed and compressed versions. Additionally we show the speed down rate in latency and the percentage of memory, compared to uncompressed, needed to operate under compressed options.*\n",
      "\n",
      "We would like to give you an extra bonus row from the above table. [Sphere](https://github.com/facebookresearch/sphere) is an open-source dataset recently released by Meta. It collects 768 dimensions and nearly a billion objects.\n",
      "Further, we may want to speed up testing by parallelizing resource allocation. For example, evaluating 4 embedding models at the same time. As discussed earlier, another interesting component to this is tuning chunking or other symbolic metadata that may come from our data importer. To paint the picture with an example, the Weaviate Verba dataset contains 3 folders of Weaviate `Blogs`, `Documentation`, and `Video` transcripts. If we want to ablate chunk sizes of 100 versus 300, it probably doesn’t make sense to re-invoke the web scraper.\n",
      "If we compress too soon, when too little data is present in uncompressed form, the centroids will be underfit and won't capture the underlying data distribution. Alternatively, compressing too late will take up unnecessary ammounts of memory prior to compression. Keep in mind that the uncompressed vectors will require more memory so if we send the entire dataset and compress only at the end we will need to host all of these vectors in memory at some point prior to compressing after which we can free that memory. Depending on the size of your vectors the compressing time could be optimally calculated but this is not a big issue either. Figure 10 shows a profile of the memory used while loading Sift1M.\n",
      "1**: *Suppose we have vectors $x$ and $y$ represented in their original space. We apply a compression function $C$ to obtain a shorter representation of $x$ ($x'$) and $y$ ($y'$) on a compressed space but would require a decompression function $C'$ from the compressed space into the original space to be able to use the original distance function. In this case we would obtain $x''$ and $y''$ from $x'$ and $y'$ respectively and apply the distance on the approximations of the original $x$ and $y$ so $d(x,y)=d(x'',y'') + \\delta$ where $\\delta$ is the distortion added to the distance calculation due of the reconstruction of the original vectors. The compression/decompression mechanisms should be such that the distortion is minimized.*\n",
      "\n",
      "![comp2](./img/image2.jpg)\n",
      "**Fig. 2**: *Suppose we have vectors $x$ and $y$ represented in their original space.\n",
      "We found that the data structures relied on dynamic allocations. So, even if we knew that an array would never be longer than 64 elements, the Go runtime could still decide to allocate an array[100] in the background when the array reaches 51 elements. To fix that, we switched to static allocations, and Weaviate instructs the Go runtime to allocate the exact number of elements. This reduced **static** memory usage even when idle. ### Results\n",
      "\n",
      "🎉 Between these two major updates, plus some smaller ones, we saw a **significant reduction in memory usage of 10-30%**🚀.\n",
      "---\n",
      "title: Building an AI-Powered Shopping Copilot with Weaviate\n",
      "slug: moonsift-story\n",
      "authors: [alea, zain]\n",
      "date: 2023-11-15\n",
      "tags: []\n",
      "image: ./img/hero.png\n",
      "description: \"UK-based startup Moonsift is harnessing the power of AI with Weaviate.\"\n",
      "---\n",
      "![hero](img/hero.png)\n",
      "\n",
      "UK-based startup Moonsift is harnessing the power of AI—using machine learning models and Weaviate’s vector database—to help online shoppers discover the products they love. <!-- truncate -->\n",
      "\n",
      "[Moonsift](https://www.moonsift.com/) offers an ecommerce browser extension for users to curate shoppable boards with products from across the internet. Stylists and curators use Moonsift to create collections, registries, and wish lists that can be shared and shopped with a simple link. While thousands of customers add products from tens of thousands of retailers per month to Moonsift, co-founders David Wood and Alex Reed have a bigger vision for improving product discoverability for online shoppers. With combined experience in natural language processing (NLP), data science, and consulting for retail brands, Wood and Reed saw how retailers unknowingly restrict their own discoverability by tailoring keywords for search engines rather than users.\n",
      "This has led to people questioning whether a Few-Shot LLM Evaluation is necessary. Due to its “good enough” status on the tipping scale, Zero-Shot LLM Evaluation may be all that is needed to be a north star for RAG system tuning. Shown below, the RAGAS score is made up of 4 prompts for Zero-Shot LLMs that evaluate the 2 metrics for generation, **Faithfulness** and **Answer Relevancy**, as well as 2 metrics for retrieval, **Context Precision** and **Context Recall**. ![Ragas-score](img/ragas-score.png)\n",
      "[Source](https://docs.ragas.io/en/latest/concepts/metrics/index.html)\n",
      "\n",
      "The transition from Zero-Shot to Few-Shot LLM Evaluation is quite straightforward. Instead of using just an instruction template, we add a few labeled examples of the relevancy of linked search results to a query.\n",
      "The temperature setting controls the amount of randomness in the output. A temperature of 0 means that the response is more predictable and will vary less. A temperature of 1 gives the model the ability to introduce randomness and creativity into its responses. Therefore, if you’re running the generative model more than once and it has a temperature of 1, the responses can vary after each rerun. Long context models are an emerging direction for choosing the LLM for your application.\n",
      "Notice that similar recall/latency results with less segments still mean better compression rate. ![res1](./img/image5.png)\n",
      "**Fig. 4**: *Time (min) to fit the Product Quantizer with 200,000 vectors and to encode 1,000,000 vectors, all compared to the recall achieved. The different curves are obtained varying the segment length (shown in the legend) from 1 to 8 dimensions per segment. The points in the curve are obtained varying the amount of centroids.*\n",
      "\n",
      "![res2](./img/image6.png)\n",
      "**Fig.\n",
      "* At the **class** level, `vectorizeClassName` will determine whether the class name is used for vectorization. * At the **property** level:\n",
      "    * `skip` will determine whether the property should be skipped (i.e. ignored) in vectorization, and\n",
      "    * `vectorizePropertyName` will determine whether the property name will be used. * The property `dataType` determines whether Weaviate will ignore the property, as it will ignore everything but `string` and `text` values. > You can read more about each variable in the [schema configuration documentation](/developers/weaviate/manage-data/collections). Let's apply this to our data to set Weaviate's vectorization behavior, then we will confirm it manually using the Cohere API as we did above.\n",
      "There are a couple of flavors to consider with this. Again reminiscent of our discussion of LLM Evaluation, we may want to use a more powerful LLM to generate the training data to produce a smaller, more economical model owned by you. Another idea could be to provide human annotations of response quality to fine-tune an LLM with instruction following. If you’re interested in fine-tuning models, check out this [tutorial](https://brev.dev/blog/fine-tuning-mistral) from Brev on how to use the HuggingFace PEFT library. ### Concluding thoughts on RAG Knobs to Tune\n",
      "In conclusion, we have presented the main knobs to tune in RAG systems:\n",
      "\n",
      "* Indexing: At the highest level, we consider when to just use brute force and when to bring in an ANN index.\n",
      "The prompt is to summarize the abstract of the two papers in one sentence. This type of summarization is very useful when scouting out new research papers. This enables us to get a quick summary of the abstract and ask questions specific to the paper. ```python\n",
      "prompt = \"\"\"\n",
      "Please summarize the following academic abstract in a one-liner for a layperson:\n",
      "\n",
      "{abstract}\n",
      "\"\"\"\n",
      "\n",
      "results = (\n",
      "    client.query.get(\"Document\", \"source\").with_generate(single_prompt=prompt).do()\n",
      ")\n",
      "\n",
      "docs = results[\"data\"][\"Get\"][\"Document\"]\n",
      "\n",
      "for doc in docs:\n",
      "    source = doc[\"source\"]\n",
      "    abstract = doc[\"_additional\"][\"generate\"][\"singleResult\"]\n",
      "    wrapped_abstract = textwrap.fill(abstract, width=80)\n",
      "    print(f\"Source: {source}\\nSummary:\\n{wrapped_abstract}\\n\")\n",
      "```\n",
      "\n",
      "<details>\n",
      "  <summary>Output</summary>\n",
      "\n",
      "```\n",
      "Source: paper01.pdf\n",
      "Summary:\n",
      "Data Augmentation is a technique that enhances the size and quality of training\n",
      "datasets for Deep Learning models, particularly useful in domains with limited\n",
      "data such as medical image analysis. ```\n",
      "```\n",
      "Source: paper02.pdf\n",
      "Summary:\n",
      "Using machine learning techniques, researchers explore predicting house prices\n",
      "with structured and unstructured data, finding that the best predictive\n",
      "performance is achieved with term frequency-inverse document frequency (TF-IDF)\n",
      "representations of house descriptions.\n",
      "## Recommendations & Wrap-up\n",
      "\n",
      "As we mentioned before, all you need to configure to enable replication is this in the collection definition:\n",
      "\n",
      "```json\n",
      "{\n",
      "  class: 'YOUR_CLASS_NAME',\n",
      "  ... replicationConfig: {\n",
      "    factor: 3,\n",
      "  }\n",
      "}\n",
      "```\n",
      "\n",
      "But what replication factor would we recommend? That’s something of a subjective question, but our starting recommendation is 3. The reason is that odd numbers are preferred for consistency so that consensus can always be reached. Higher factors are also possible, but this is more of a measure to scale query throughput, rather than lead to more availability.\n",
      "For this experiment we have added 200,000 vectors using the normal HNSW algorithm, then we switched to compressed and added the remaining 800,000 vectors.*\n",
      "\n",
      "![perf2](./img/image13.png)\n",
      "**Fig. 12**: *The chart shows Recall (vertical axis) Vs Indexing time (in minutes, on the horizontal axis). For this experiment we have added 200,000 vectors using the normal HNSW algorithm, then we switched to compressed and added the remaining 800,000 vectors.*\n",
      "\n",
      "![perf3](./img/image14.png)\n",
      "**Fig. 13**: *The chart shows Recall (vertical axis) Vs Latency (in microseconds, on the horizontal axis). For this experiment we have added 200,000 vectors using the normal HNSW algorithm, then we switched to compressed and added the remaining 800,000 vectors.*\n",
      "\n",
      "![perf4](./img/image15.png)\n",
      "**Fig.\n",
      "We will review and help you out in the process. 💚\n",
      "\n",
      "You can also contribute by adding your own Weaviate examples. If you have other great ideas for contributions, let us know on our [Discourse](https://forum.weaviate.io/) and [Slack](https://weaviate.slack.com/) channels, and we will figure out how to highlight it in Hacktoberfest. You don't need to be an expert to contribute to these demo projects!\n",
      "\n",
      "\n",
      "## Resources to Get Started\n",
      "\n",
      "We're thrilled to help you make your first open-source contribution! Here are some helpful resources to kickstart your journey:\n",
      "\n",
      "What is Open Source, and how do you contribute to it? - 🎯 [What is Open Source](https://www.digitalocean.com/community/tutorials/what-is-open-source)\n",
      "- 🎯 [Introduction to GitHub and Open-Source Projects](https://www.digitalocean.com/community/tutorial_series/an-introduction-to-open-source)\n",
      "- 🎯 [How to Contribute to Open Source](https://opensource.guide/how-to-contribute/)\n",
      "- 🎯 [GitHub Contribution Guide by Hugging Face](https://www.notion.so/Contribution-Guide-19411c29298644df8e9656af45a7686d?pvs=21)\n",
      "- 🎯 [How to Use Git](https://www.digitalocean.com/community/cheatsheets/how-to-use-git-a-reference-guide)\n",
      "- 🎯 [Weaviate Contributor Guide](https://weaviate.io/developers/contributor-guide)\n",
      "\n",
      "If you're new to Weaviate, get up and running quickly with these beginner-friendly guides:\n",
      "\n",
      "- [Quickstart Guide](https://weaviate.io/developers/weaviate/quickstart) 🚀\n",
      "- [Weaviate Academy](https://weaviate.io/developers/academy) 🎓\n",
      "\n",
      "Dive deeper into specific topics with these detailed guides:\n",
      "\n",
      "- [How-to Search Guides](https://weaviate.io/developers/weaviate/search) 🔍\n",
      "- [Keyword, Vector, Hybrid, and Generative Search](https://github.com/weaviate-tutorials/generative-search/blob/main/GenerativeSearchDemo.ipynb) 🔍\n",
      "- [How-to Manage Data (CRUD Operations)](https://weaviate.io/developers/weaviate/manage-data) 💾\n",
      "- [Tutorial: Importing Data with Your Own Vectors](https://weaviate.io/developers/weaviate/tutorials/wikipedia) 📊\n",
      "- [Weaviate Architecture Concepts](https://weaviate.io/developers/weaviate/concepts#weaviate-architecture) 🏛️\n",
      "\n",
      "Join one of our [workshops](https://weaviate.io/learn/workshops) for an introduction to Weaviate.\n",
      "This is done with:\n",
      "\n",
      "```python\n",
      "from unstructured.partition.pdf import partition_pdf\n",
      "\n",
      "elements = partition_pdf(filename=\"../data/paper01.pdf\")\n",
      "```\n",
      "\n",
      "Now, if we want to see all of the elements that Unstructured found, we run:\n",
      "\n",
      "```python\n",
      "titles = [elem for elem in elements if elem.category == \"Title\"]\n",
      "\n",
      "for title in titles:\n",
      "    print(title.text)\n",
      "```\n",
      "\n",
      "<details>\n",
      "  <summary>Response from Unstructured</summary>\n",
      "\n",
      "A survey on Image Data Augmentation for Deep Learning\n",
      "Abstract\n",
      "Introduction\n",
      "Background\n",
      "Image Data Augmentation techniques\n",
      "Data Augmentations based on basic image manipulations\n",
      "Flipping\n",
      "Color space\n",
      "Cropping\n",
      "Rotation\n",
      "Translation\n",
      "Noise injection\n",
      "Color space transformations\n",
      "Geometric versus photometric transformations\n",
      "Kernel filters\n",
      "Mixing images\n",
      "Random erasing\n",
      "A note on combining augmentations\n",
      "Data Augmentations based on Deep Feature space augmentation\n",
      "Data Augmentations based on Deep Learning\n",
      "Feature space augmentation\n",
      "Adversarial training\n",
      "GAN‑based Data Augmentation\n",
      "Generated images\n",
      "Neural Style Transfer\n",
      "Meta learning Data Augmentations\n",
      "Comparing Augmentations\n",
      "Design considerations for image Data Augmentation\n",
      "Test-time augmentation\n",
      "Curriculum learning\n",
      "Resolution impact\n",
      "Final dataset size\n",
      "Alleviating class imbalance with Data Augmentation\n",
      "Discussion\n",
      "Future work\n",
      "Conclusion\n",
      "Abbreviations\n",
      "Acknowledgements\n",
      "Authors’ contributions\n",
      "Funding\n",
      "References\n",
      "Publisher’s Note\n",
      "\n",
      "</details>\n",
      "\n",
      "If we want to store the elements along with the content, you run:\n",
      "\n",
      "```python\n",
      "import textwrap\n",
      "\n",
      "narrative_texts = [elem for elem in elements if elem.category == \"NarrativeText\"]\n",
      "\n",
      "for index, elem in enumerate(narrative_texts[:5]):\n",
      "    print(f\"Narrative text {index + 1}:\")\n",
      "    print(\"\\n\".join(textwrap.wrap(elem.text, width=70)))\n",
      "    print(\"\\n\" + \"-\" * 70 + \"\\n\")\n",
      "```\n",
      "\n",
      "You can then take this data, vectorize it and store it in Weaviate. ![PDFs to Weaviate](./img/Weaviate-ingesting-dark.png#gh-dark-mode-only)\n",
      "![PDFs to Weaviate](./img/Weaviate-ingesting-light.png#gh-light-mode-only)\n",
      "\n",
      "## End-to-End Example\n",
      "Now that we’ve introduced the basics of using Unstructured, we want to provide an end-to-end example. We’ll read a folder containing the two research papers, extract their abstracts and store them in Weaviate. Starting with importing the necessary libraries:\n",
      "\n",
      "```python\n",
      "from pathlib import Path\n",
      "import weaviate\n",
      "from weaviate.embedded import EmbeddedOptions\n",
      "import os\n",
      "```\n",
      "\n",
      "In this example, we are using [Embedded Weaviate](/developers/weaviate/installation/embedded). You can also run it on [WCS](https://console.weaviate.cloud) or [docker](/developers/weaviate/installation/docker-compose).\n",
      "Training a single, jack-of-all-modalities model is very difficult. Current multimodal approaches like [ImageBind](https://arxiv.org/abs/2305.05665) from FAIR, Meta AI(which combines image, speech, text, video, motion and depth/thermal data) approach this problem by taking separate specialist pre-trained models for each modality and then finetuning them to bind their latent space representations using a contrastive loss function; which essentially pulls together representations of similar examples across modalities closer together and pushes apart distinct examples in a joint vector space. The key insight is that all combinations of paired modalities are not necessary to train such a joint embedding, and only image-paired data is sufficient to align and bind the modalities together. The limitation however is that adding and finetuning a separate pretrained model for every modality or task becomes prohibitively expensive and is not scalable. ### 3.\n",
      "Each query engine has its own strengths in the information retrieval process, let’s dive into a couple of them and how we might think about evaluation. <img\n",
      "  src={require('./img/sub-question.png').default}\n",
      "  alt=\"Sub Question Query Engine\"\n",
      "  style={{ maxWidth: \"60%\" }}\n",
      "/>\n",
      "\n",
      "Multi-hop query engines (otherwise known as [sub question query engine](https://gpt-index.readthedocs.io/en/latest/examples/query_engine/sub_question_query_engine.html)) are great at breaking down complex questions into sub-questions. In the visual above, we have the query “What is Ref2Vec in Weaviate?” To answer this question, you need to know what Ref2Vec and Weaviate are separately. Therefore, two calls will need to be made to your database to retrieve relevant context based on the two questions. The two answers are then combined to generate one output.\n",
      "When we rerun the original test (with ten parallel aggregations), we saw the memory consumption drop to 30GB (vs 200GB). <!-- TODO: add a quote from Juraj\n",
      "But don't take our word for it, this was from a test run by one of our community members…\n",
      " -->\n",
      "\n",
      "## New distance metrics\n",
      "\n",
      "![Hamming and Manhattan distance metrics](./img/distance-metrics.png)\n",
      "\n",
      "Thanks to the community contributions from [Aakash Thatte](https://github.com/sky-2002), Weaviate `v1.15` adds two new distance metrics: **Hamming** distance and **Manhattan** distance. In total, you can now choose between five various distance metrics to support your datasets. Check out the [metrics documentation page](/developers/weaviate/config-refs/distances#distance-implementations-and-optimizations) for a complete overview of all available metrics in Weaviate. ### Hamming distance\n",
      "The Hamming distance is a metric for comparing two numerical vectors.\n",
      "6**: *Time (min) to fit the Product Quantizer with 200,000 vectors and to encode 9,990,000 vectors, all compared to the recall achieved. The different curves are obtained varying the segment length (shown in the legend) from 1 to 6 dimensions per segment. The points in the curve are obtained varying the amount of centroids.*\n",
      "\n",
      "![res4](./img/image8.png)\n",
      "**Fig. 7**: *Average time (microseconds) to calculate distances from query vectors to all 9,990,000 vectors compared to the recall achieved. The different curves are obtained varying the segment length (shown in the legend) from 1 to 6 dimensions per segment.\n",
      "To see a list of the newly spun up nodes, run:\n",
      "\n",
      "```shell\n",
      "kubectl get nodes -o wide\n",
      "```\n",
      "\n",
      "You should see an output similar to the following, indicating that three nodes are up and onto which you can deploy Weaviate:\n",
      "\n",
      "```shell\n",
      "NAME           STATUS   ROLES           AGE    VERSION   INTERNAL-IP    EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION     CONTAINER-RUNTIME\n",
      "minikube       Ready    control-plane   134m   v1.27.3   192.168.49.2   <none>        Ubuntu 22.04.2 LTS   5.15.49-linuxkit   docker://24.0.4\n",
      "minikube-m02   Ready    <none>          134m   v1.27.3   192.168.49.3   <none>        Ubuntu 22.04.2 LTS   5.15.49-linuxkit   docker://24.0.4\n",
      "minikube-m03   Ready    <none>          133m   v1.27.3   192.168.49.4   <none>        Ubuntu 22.04.2 LTS   5.15.49-linuxkit   docker://24.0.4\n",
      "```\n",
      "\n",
      "Now, add the Weaviate helm repository to your local helm configuration by running:\n",
      "\n",
      "```shell\n",
      "helm repo add weaviate https://weaviate.github.io/weaviate-helm\n",
      "```\n",
      "\n",
      "And save the default configuration with:\n",
      "\n",
      "```shell\n",
      "helm show values weaviate/weaviate > values.yaml\n",
      "```\n",
      "\n",
      "Edit `values.yaml` by changing the root-level configuration `replicas: 1` for the root image to `replicas: 3`, and save it. ```yaml\n",
      "... # Scale replicas of Weaviate. Note that as of v1.8.0 dynamic scaling is limited\n",
      "# to cases where no data is imported yet. Scaling down after importing data may\n",
      "# break usability.\n",
      "For sure it might not be for everybody and every use case. But if you are using Weaviate at scale, in production, we believe enabling it will add significant value and encourage you to consider its use.\n",
      "You can even run transformer models locally with [`text2vec-transformers`](/developers/weaviate/modules/retriever-vectorizer-modules/text2vec-transformers), and modules such as [`multi2vec-clip`](/developers/weaviate/modules/retriever-vectorizer-modules/multi2vec-clip) can convert images and text to vectors using a CLIP model. But they all perform the same core task—which is to represent the “meaning” of the original data as a set of numbers. And that’s why semantic search works so well. import WhatNext from '/_includes/what-next.mdx'\n",
      "\n",
      "<WhatNext />\n",
      "The fact that the graph is still in memory makes it hard to see the difference between those different levels of compression. The more we compress the lower the recall we would expect. Let us discuss the lowest level of compression along with some expectations. For Sift1M we would require roughly 1277 MB to 1674 MB of memory to index our data using uncompressed HNSW. This version would give us recall ranging from 0.96811 to 0.99974 and latencies ranging from 293 to 1772 microseconds.\n",
      "<br></br>\n",
      "\n",
      "### Cloud Operations & Scaling\n",
      "![cloud operations scaling](./img/cloud-operations-scaling.png)\n",
      "\n",
      "When we introduced [Replication](/developers/weaviate/concepts/replication-architecture) to Weaviate in late 2022, we celebrated a significant milestone. It’s never been easier to achieve a highly available setup, and you can even dynamically scale your setup to increase throughput. 2023 is all about improving your cloud and operations experience. We will give you more control over [how to structure your workloads](https://github.com/weaviate/weaviate/issues/2586) in a distributed setup and more [flexibility to adapt to your ever-changing needs](https://github.com/weaviate/weaviate/issues/2228). And, of course, we’re constantly working on making your distributed cluster [even more resilient](https://github.com/weaviate/weaviate/issues/2405).\n",
      "- How much do we save in memory requirements? ### Performance Results\n",
      "\n",
      "In the following figures we show the performance of HNSW+PQ on the three databases used above. Notice how compressing with KMeans keeps the recall closer to the uncompressed results. Compressing too aggressively (KMeans with a few dimensions per segment) improves memory, indexing and latency performance but it rapidly destroys the recall so we recommend using it discreetly. Notice also that KMeans encoding with as many segments as dimensions ensures a 4 to 1 compression ratio.\n",
      "### Implementation observations\n",
      "\n",
      "We initially implemented the Vamana algorithm as described, resulting in very good recall results. Yet the latency was not good at all. We have since realized that the performance decay was due to many set operations making the algorithm perform poorly as is. In our revised implementation, we have modified the algorithm a bit to keep a copy of visited and current nodes on a single sorted list. Also, as the parameter L grows, the search on the sets becomes more expensive, so we already decided to keep a bit-based representation of the vectors residing on the sets, which made a huge impact performance-wise.\n",
      "There was however one point in the API where reusing IDs between classes was causing serious issues. Most noticeable this was for the [v1/objects/{id}](/developers/weaviate/api/rest/objects) REST endpoints. If you wanted to retrieve, modify or delete a data object by its ID, you would just need to specify the ID, without specifying the class name. So if the same ID exists for objects in multiple classes (which is fine because of the namespaces per class), Weaviate would not know which object to address and would address all objects with that ID instead. I.e. if you tried to delete an object by ID, this would result in the deletion of all objects with that ID.\n",
      "Back in 2020, this required humans to have conversations with the chatbot and manually assign these ratings. While it is good to avoid vague responses, it is equally important to avoid the LLM from **hallucinating**. Hallucination refers to the LLM generating a response that is not grounded in actual facts or the provided context. [LlamaIndex](https://docs.llamaindex.ai/en/latest/examples/evaluation/faithfulness_eval.html) measures this with a `FaithfulnessEvaluator` metric. The score is based on whether the response matches the retrieved context.\n",
      "The compression is carried out by using predefined centers, which we will explain shortly. If we aim to compress each segment down to 8 bits (one byte) of memory, we might have 256 (total combinations with 8 bits) predefined centers per segment. When compressing a vector we would go segment-by-segment assigning a byte representing the index of the predefined center. The segmentation and compression process is demonstrated in Figure 3 below. ![pq](./img/image3.jpg)\n",
      "**Fig.\n",
      "It’s just that easy. Before you rush off to switch on replication, though, stick with us to read about the trade-offs and our recommendations. 😉\n",
      "\n",
      "## Trade-offs & discussions\n",
      "\n",
      "While replication and high availability are wonderful, we won’t quite pretend that it comes for free. Having additional replicas of course means that there are more tenants and objects overall. Although they are duplicated, they are just as *real* as objects as any others.\n",
      "## Exploring the Power of Vector Databases\n",
      "\n",
      "The year 2023 was all about dynamic experimentation at Weaviate. Vector databases became a strong and recognized foundation in building ever more effective AI applications, enabling **chatbots,** **agents,** and **advanced** **search systems**. ### Online Hackathons\n",
      "\n",
      "Our 2023 global online hackathons proved to be vibrant innovation hubs, fostering diversity and inclusion in collaborative work. We teamed up with friends from [Cohere](https://cohere.com/), [LangChain](https://www.langchain.com/), [AutoGPT](https://autogpt.net/), [lablab.ai](https://lablab.ai/), [SuperAGI](https://superagi.com/), and many others. ![hackathons](img/hackathons.png)\n",
      "\n",
      "### In-person Hackathons\n",
      "Whether you're a beginner just diving into the world of coding, a passionate AI enthusiast, or a seasoned expert in the field, in-person events create a burst of energy and creativity into everyone's personal AI journey.\n",
      "---\n",
      "title: Authentication in Weaviate (videos)\n",
      "slug: authentication-in-weaviate\n",
      "authors: [jp]\n",
      "date: 2023-04-25\n",
      "image: ./img/hero.png\n",
      "tags: ['concepts']\n",
      "description: \"Videos on authentication: an overview, how to log in, how to set it up, and core concepts - including recommendations.\"\n",
      "\n",
      "---\n",
      "\n",
      "![Authentication in Weaviate](./img/hero.png)\n",
      "\n",
      "<!-- truncate -->\n",
      "\n",
      "import ReactPlayer from 'react-player/lazy'\n",
      "\n",
      "## Overview\n",
      "\n",
      "Authentication is one of those topics that we get quite a few questions about. And we can see why. It's a big, complex topic, and even within Weaviate, there are many options available which can make it seem quite confusing. The core concept of authentication is relatively simple. When a client (e.g. a Weaviate client) sends a request to a server (e.g. a Weaviate database), it includes a \"secret\" that provides some assurances to Weaviate as to who that request is coming from, so that it can operate on that information.\n",
      "Weaviate generates vector embeddings using [modules](/developers/weaviate/modules/retriever-vectorizer-modules) (OpenAI, Cohere, Google PaLM etc.), and conveniently stores both objects and vectors in the same database. For example, vectorizing the two words above might result in:\n",
      "\n",
      "```text\n",
      "cat = [1.5, -0.4, 7.2, 19.6, 3.1, ..., 20.2]\n",
      "kitty = [1.5, -0.4, 7.2, 19.5, 3.2, ..., 20.8]\n",
      "```\n",
      "\n",
      "These two vectors have a very high similarity. In contrast, vectors for “banjo” or “comedy” would not be very similar to either of these vectors. To this extent, vectors capture the semantic similarity of words. Now that you’ve seen what vectors are, and that they can represent meaning to some extent, you might have further questions.\n",
      "If that is something up your street, check out [the distancer code on github](https://github.com/weaviate/weaviate/tree/master/adapters/repos/db/vector/hnsw/distancer), to see the implementation of other metrics. Just make sure to include plenty of tests. Remember: \"reliability, reliability, reliability\". ## Updated API endpoints to manipulate data objects of specific class\n",
      "![Updated API endpoints](./img/updated-API-endpoints.png)\n",
      "\n",
      "The REST API CRUD operations now require you to use both an **object ID** and the target **class name**.<br/>\n",
      "This ensures that the operations are performed on the correct objects. ### Background\n",
      "One of Weaviate's features is full CRUD support.\n",
      "![workshops](img/workshops_3.png)\n",
      "\n",
      "## Building a Global Community\n",
      "\n",
      "The heart of Weaviate lies in its **community**. Our strength comes from the combined efforts of our team and users, whether they're engaging in the [Community Slack](https://weaviate.slack.com/), participating in meetups, or using Weaviate in their projects. This year, we introduced the **Weaviate Hero Program**, an initiative by [Marion](https://www.linkedin.com/in/marionnehring/), our Community Manager, to honor those significantly contributing to our community's growth and success. ### Meetups and Events\n",
      "\n",
      "**Weaviate World Tour - End of Year Edition:** To ensure everyone can benefit from the knowledge, we decided to share our knowledge and connect, collaborate, and network with community members around the globe. We introduced our Weaviate World Tour: Year-End Special Edition! bringing tech experts to community events in Amsterdam, Berlin, London, San Francisco, and New York to hundreds of developers, data scientists, and AI enthusiasts.\n",
      "Our new schema is below - note the commented lines:\n",
      "\n",
      "```python\n",
      "question_class = {\n",
      "    \"class\": \"Question\",\n",
      "    \"description\": \"Details of a Jeopardy! question\",\n",
      "    \"moduleConfig\": {\n",
      "        \"text2vec-cohere\": {  # The vectorizer name - must match the vectorizer used\n",
      "            \"vectorizeClassName\": False,  # Ignore class name\n",
      "        },\n",
      "    },\n",
      "    \"properties\": [\n",
      "        {\n",
      "            \"name\": \"answer\",\n",
      "            \"description\": \"What the host prompts the contestants with.\",\n",
      "            \"dataType\": [\"string\"],\n",
      "            \"moduleConfig\": {\n",
      "                \"text2vec-cohere\": {\n",
      "                    \"skip\": False,  # Do not skip class\n",
      "                    \"vectorizePropertyName\": False  # Ignore property name\n",
      "                }\n",
      "            }\n",
      "        },\n",
      "        {\n",
      "            \"name\": \"question\",\n",
      "            \"description\": \"What the contestant is to provide.\",\n",
      "            \"dataType\": [\"string\"],\n",
      "            \"moduleConfig\": {\n",
      "                \"text2vec-cohere\": {\n",
      "                    \"skip\": False,  # Do not skip class\n",
      "                    \"vectorizePropertyName\": True  # Do not ignore property name\n",
      "                }\n",
      "            }\n",
      "        },\n",
      "    ]\n",
      "}\n",
      "client.schema.create_class(question_class)\n",
      "```\n",
      "\n",
      "The schema is defined such that at least some of the options, such as `moduleConfig`/`text2vec-cohere` /`vectorizeClassName` and `properties`/`moduleConfig`/`text2vec-cohere`/`vectorizePropertyName` differ from their defaults. And as a result, a `nearVector` search with the previously-matching Cohere API vector is now at a distance of `0.00395`. To get this back down to zero, we must revise the text generation pipeline to match the schema. Once we've done that, which looks like this:\n",
      "\n",
      "```python\n",
      "str_in = ''\n",
      "for k in sorted(input_props.keys()):\n",
      "    v = input_props[k]\n",
      "    if type(v) == str:\n",
      "        if k == 'question':\n",
      "            str_in += k + ' '\n",
      "        str_in += v + ' '\n",
      "str_in = str_in.lower().strip()\n",
      "```\n",
      "\n",
      "Searching with the vector generated from this input, the closest matching object in Weaviate once again has a distance of zero. We've come full circle 🙂.\n",
      "- **I don’t know how to write code. Can I still contribute?** Yes, of course! You can make no-code contributions, e.g., by updating the README.md files. If you want to learn how to write code with a concrete example, we can help you find a good issue. Just ping us on our [Discourse](https://forum.weaviate.io/) or [Slack](https://weaviate.slack.com/) channels. ---\n",
      "\n",
      "Happy hacking, and let's make Hacktoberfest 2023 a memorable one together! 🚀\n",
      "\n",
      "Jump right in and have a look at our [example use cases and demos](https://weaviate.io/developers/weaviate/more-resources/example-use-cases) page.\n",
      "Tech giants like Google, AWS, or Microsoft Azure offer their vector-search capabilities to customers willing to upload their data. But there's now an ecosystem of newer companies with AI-first specific (often open-source) solutions and vector-search capabilities that customers can run on a SaaS basis or on their own systems. ## The AI-first Database Ecosystem\n",
      "The companies that make up this ecosystem provide specialized services that overlap to various degrees. Combined, four sub-groups make up the ecosystem. 1.\n",
      "Though we still have a ways to go on this journey, as noted earlier, but what we've accomplished thus far can add loads of value to Weaviate users. This solution already allows for significant savings in terms of memory. When compressing vectors ranging from 100 to 1000 dimensions you can save half to three fourths of the memory you would normally need to run HNSW. This saving comes with a little cost in recall or latency. The indexing time is also larger but to address this we've cooked up a second encoder, developed specially by Weaviate, based on the distribution of the data that will reduce the indexing time significantly.\n",
      "Embedding providers (e.g., Hugging Face or OpenAI)\n",
      "1. Neural framework (e.g., deepset or Jina)\n",
      "1. Feature stores (e.g., FeatureBase, FeatureForm or Tecton)\n",
      "1. Vector databases (e.g., Weaviate or Vertex)\n",
      "\n",
      "While the number of data that companies are collecting in their data warehouses keeps growing, the need for better, more efficient searches keeps growing too. The more data we collect, the more complex searching through it becomes.\n",
      "---\n",
      "title: How to run an embedded vector database in 10 lines of code\n",
      "slug: embedded-local-weaviate\n",
      "authors: [dan]\n",
      "date: 2023-06-06\n",
      "image: ./img/hero.png\n",
      "tags: ['how-to']\n",
      "description: \"The Weaviate server can be run locally directly from client code\"\n",
      "\n",
      "---\n",
      "\n",
      "import Tabs from '@theme/Tabs';\n",
      "import TabItem from '@theme/TabItem';\n",
      "import FilteredTextBlock from '@site/src/components/Documentation/FilteredTextBlock';\n",
      "import PyCode from '!!raw-loader!/_includes/code/embedded.py';\n",
      "import TSCode from '!!raw-loader!/_includes/code/embedded.ts';\n",
      "\n",
      "Yes, 10 Python lines of code, generously formatted with whitespace. Or 14 for TypeScript. Oh, and all your data stays private locally, and we don't charge you anything. We're also going to build a useful example, illustrating a testing scenario. Here's how.\n",
      "Developers who want to build AI-powered applications can now skip the tedious process of complex training strategies. Now you can simply take models off-the-shelf and plug them into your apps. Applying a ranking model to hybrid search results is a promising approach to keep pushing the frontier of zero-shot AI. Imagine we want to retrieve information about the Weaviate Ref2Vec feature. If our application is using the Cohere embedding model, it has never seen this term or concept.\n",
      "![animation](./img/animation.png)\n",
      "\n",
      "The blog post included this great visual to help with the visualization of combining Bi-Encoders and Cross-Encoders. This fishing example explains the concept of coarse-grained retrieval (fishing net = vector search / bm25) and manual inspection of the fish (fishermen = ranking models). Depicted with manual inspection of fish, the main cost of ranking models is speed. In March, Bob van Luijt appeared on a Cohere panel to discuss [“AI and The Future of Search”](https://twitter.com/cohereai/status/1636396916157079554?s=46&t=Zzg6vgh4rwmYEkdV-3v5gg). Bob explained the effectiveness of combining zero-shot vector embedding models from providers such as Cohere, OpenAI, or HuggingFace with BM25 sparse search together in Hybrid Search.\n",
      "This delta refers to the error we introduce due to the compression - we are using a lossy compression process. As we mentioned before, we are lowering the accuracy of our data, meaning the distance we calculate is a bit distorted; this distortion is exactly what we represent using delta. We don't aim to calculate such an error, nor to try to correct it. We should however acknowledge it and try to keep it low. ![comp1](./img/image1.jpg)\n",
      "**Fig.\n",
      "How even with the most illustrious descriptions, shoppers struggle to find the products they’re looking for. They knew that retail discoverability wasn’t what it could be, and built Moonsift to improve it. > From the beginning, Wood and Reed’s vision was to use the power of AI to help shoppers more easily discover the products they love. ## Collecting data to enable understanding \n",
      "The first generation of Moonsift has been a stepping stone toward this vision. In order to create a comprehensive AI shopping copilot, they needed product data from retailers along with cross-retailer shopping data to enable understanding of the discovery process and train machine learning models.\n",
      "Now armed with queries, answers, gold documents, and negatives, ARES fine-tunes lightweight classifiers for **context relevance**, **answer faithfulness**, and **answer relevance**. The authors experiment with fine-tuning [DeBERTa-v3-large](https://huggingface.co/microsoft/deberta-v3-large), which contains a more economical 437 million parameters, with each classifier head sharing the base language model, adding 3 total classification heads. The ARES system is then evaluated by dividing the synthetic data into a train-test split and comparing the fine-tuned judges with zero-shot and few-shot GPT-3.5-turbo-16k judges, finding that the fine-tuned models perform significantly better. For further details, such as a novel use of confidence intervals with prediction powered inference (PPI) and more experimental details, please see the paper from [Saad-Falcon et al](https://arxiv.org/abs/2311.09476). To better understand the potential impact of LLMs for evaluation, we will continue with a tour of the existing methods for benchmarking RAG systems and how they are particularly changed with LLM Evaluation.\n",
      "Weaviate `v1.20` - coming in July 2023 - changes this once and for all: Native multi-tenancy support that scales to millions of tenants with 10s of thousands of active tenants per node. Yet scale is not the only point that makes the new multi-tenancy feature great; we put a lot of emphasis on compliance and a smooth UX. GDPR-compliant deletes with one command are just one of the many features. Let me walk you through what’s coming in the next Weaviate release and show you why I’m incredibly excited about this one. ## The need for multi-tenancy\n",
      "We define multi-tenancy as the need to serve multiple distinct users or user groups from a single application.\n",
      "These embeddings better reflect the polysemantic nature of words, which can only be disambiguated when they are considered in context. Some of the potential downsides include:\n",
      "* increased compute requirements: fine-tuning transformer models is much slower (on the order of hours vs. minutes)\n",
      "* increased memory requirements: context-sensitivity greatly increases memory requirements, which often leads to limited possible input lengths\n",
      "\n",
      "Despite these downsides, transformer models have been wildly successful. Countless text vectorizer models have proliferated over the recent past. Plus, many more vectorizer models exist for other data types such as audio, video and images, to name a few.\n",
      "CRUD operations enable the mutability of data objects and their vectors, which is a key difference between a vector database and an ANN library. In Weaviate, every data object has an ID (UUID). This ID is stored with the data object in a key-value store. IDs don't have to be globally unique, because in Weaviate [classes](/developers/weaviate/manage-data/collections) act as namespaces. While each class has a different [HNSW index](/developers/weaviate/concepts/vector-index#hnsw), including the store around it, which is isolated on disk.\n",
      "🤔 With this, you can get more out of your existing setups and push your Weaviate instances to do more, or you could save on the resources. ## Better control over Garbage Collector\n",
      "\n",
      "![GOMEMLIMIT](./img/gomemlimit.jpg)\n",
      "\n",
      "Weaviate is built from the ground up in Go, which allows for building very performant and memory-safe applications. Go is a garbage-collected language. > *A quick refresher:*<br/>\n",
      "> In a garbage-collected language, such as Go, C#, or Java, the programmer doesn't have to deallocate objects manually after using them. Instead, a GC cycle runs periodically to collect memory no longer needed and ensure it can be assigned again.\n",
      "Imagine a fictional company called ACME Accounting Group that offers online accounting services that use AI to make accounting easy and fun. The company has over one million customers. Each customer is a company that can have many users and even more documents. Alice, who works for AliceCorp, should never be able to see the accounting information of Bob, who works for BobInc. Therefore, AliceCorp and BobInc are tenants from the perspective of ACME Accounting.\n",
      "This demo is also using OpenAI for vectorization; you can choose another `text2vec` module [here](/developers/weaviate/modules/retriever-vectorizer-modules). ```python\n",
      "client = weaviate.Client(\n",
      "    embedded_options=EmbeddedOptions(\n",
      "        additional_env_vars={\"OPENAI_APIKEY\": os.environ[\"OPENAI_APIKEY\"]}\n",
      "    )\n",
      ")\n",
      "```\n",
      "\n",
      "### Configure the Schema\n",
      "\n",
      "Now we need to configure our schema. We have the `document` class along with the `abstract` property. ```python\n",
      "client.schema.delete_all()\n",
      "\n",
      "schema = {\n",
      "    \"class\": \"Document\",\n",
      "    \"vectorizer\": \"text2vec-openai\",\n",
      "    \"properties\": [\n",
      "        {\n",
      "            \"name\": \"source\",\n",
      "            \"dataType\": [\"text\"],\n",
      "        },\n",
      "        {\n",
      "            \"name\": \"abstract\",\n",
      "            \"dataType\": [\"text\"],\n",
      "            \"moduleConfig\": {\n",
      "                \"text2vec-openai\": {\"skip\": False, \"vectorizePropertyName\": False}\n",
      "            },\n",
      "        },\n",
      "    ],\n",
      "    \"moduleConfig\": {\n",
      "        \"generative-openai\": {},\n",
      "        \"text2vec-openai\": {\"model\": \"ada\", \"modelVersion\": \"002\", \"type\": \"text\"},\n",
      "    },\n",
      "}\n",
      "\n",
      "client.schema.create_class(schema)\n",
      "```\n",
      "\n",
      "### Read/Import the documents\n",
      "\n",
      "Now that our schema is defined, we want to build the objects that we want to store in Weaviate. We wrote a helper class,  `AbstractExtractor` to aggregate the element class.\n",
      "Use the hashtag #hacktoberfest2023 for increased visibility. :::\n",
      "\n",
      "\n",
      "## FAQ\n",
      "\n",
      "- **Will this count towards Hacktoberfest?** Yes, it definitely does! If your PR/MR is created between **October 1** and **October 31** (in any time zone, UTC-12 thru UTC+14), we will add the \"HACKTOBERFEST-ACCEPTED\" label to it. - **Where do I get help?** For any questions or assistance, contact us on our [Discourse](https://forum.weaviate.io/) and [Slack](https://weaviate.slack.com/) channels. - **I have a cool contribution idea. Can I still participate?** Awesome! Connect with us on our [Discourse](https://forum.weaviate.io/) or [Slack](https://weaviate.slack.com/) channels and we will figure it out.\n",
      "This case is quite similar to our discussion of Multi-Index Routing and we can similarly evaluate generations with a prompt that explains the needs for SQL and Vector Databases and then asks the LLM whether the router made the right decision. We can also use the RAGAS Context Relevance score for the results of the SQL query. <img\n",
      "  src={require('./img/sql-router.png').default}\n",
      "  alt=\"SQL Router Query Engine\"\n",
      "  style={{ maxWidth: \"60%\" }}\n",
      "/>\n",
      "\n",
      "Concluding our discussion of “From RAG to Agent Evaluation”, we believe that it is still too early to tell what the common patterns will be for agent use. We have intentionally shown the multi-hop query engine and query router because these are relatively straightforward to understand. Once we add more open-ended planning loops, tool use and the associated evaluation of how well the model can format API requests to the tool, and more meta internal memory management prompts such as the ideas in MemGPT, it is very difficult to provide a general abstraction around how Agents will be evaluated.\n",
      "---\n",
      "title: Why is Vector Search so fast? slug: why-is-vector-search-so-fast\n",
      "authors: [laura]\n",
      "date: 2022-09-13\n",
      "tags: ['search']\n",
      "image: ./img/hero.png\n",
      "description: \"Vector Databases can run semantic queries on multi-million datasets in milliseconds. How is that possible?\"\n",
      "---\n",
      "![Why is Vector Search so fast?](./img/hero.png)\n",
      "\n",
      "<!-- truncate -->\n",
      "\n",
      "## Why is this so incredibly fast? Whenever I talk about vector search, I like to demonstrate it with an example of a semantic search. To add the wow factor, I like to run my queries on a Wikipedia dataset, which is populated with over 28 million paragraphs sourced from Wikipedia.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
