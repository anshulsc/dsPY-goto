{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSPY SETTINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install dspy-ai==2.1.9 weaviate-client==3.26.2 > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "openai.api_key = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anshulsingh/miniconda3/envs/llms/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/anshulsingh/miniconda3/envs/llms/lib/python3.11/site-packages/weaviate/warnings.py:158: DeprecationWarning: Dep016: You are using the Weaviate v3 client, which is deprecated.\n",
      "            Consider upgrading to the new and improved v4 client instead!\n",
      "            See here for usage: https://weaviate.io/developers/weaviate/client-libraries/python\n",
      "            \n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import dspy \n",
    "from dspy.retrieve.weaviate_rm import WeaviateRM\n",
    "import weaviate\n",
    "import openai\n",
    "\n",
    "llm = dspy.OpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "weaviate_client = weaviate.Client(url = \"URL\",\n",
    "                                  auth_client_secret=weaviate.auth.AuthApiKey(\"API\"),\n",
    "                                  additional_headers={\n",
    "                                      'X-Cohere-Api-Key': \"API\"\n",
    "                                  }\n",
    ")\n",
    "                              \n",
    "retreiver_model = WeaviateRM(\"WeaviateBlogChunk\", weaviate_client=weaviate_client)\n",
    "dspy.settings.configure(lm=llm, rm=retreiver_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Wires of thought entwine,\\nConnections spark and align,\\nNeural networks divine.']\n",
      "['In the realm of silicon minds, they learn and grow,\\nNeural networks, in data streams, secrets they sow.\\nA dance of algorithms, in binary codes they flow.']\n"
     ]
    }
   ],
   "source": [
    "print(dspy.settings.lm(\"Write a 3 line poem about neural networks\"))\n",
    "context_example = dspy.OpenAI(model=\"gpt-4\")\n",
    "\n",
    "with dspy.context(llm=ConnectionAbortedError):\n",
    "    print(context_example(\"Write a 3 line poem about neural networks\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Datasets With dspY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Why would I use Weaviate as my vector database?',\n",
       " 'What is the difference between Weaviate and for example Elasticsearch?',\n",
       " 'Do you offer Weaviate as a managed service?',\n",
       " 'How should I configure the size of my instance?',\n",
       " 'Do I need to know about Docker (Compose) to use Weaviate?']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "f = open(\"faq.md\")\n",
    "markdown_content = f.read()\n",
    "\n",
    "\n",
    "def parse_questions(markdown_content):\n",
    "    question_pattern = r'#### Q: (.+?)\\n'\n",
    "\n",
    "    questions = re.findall(question_pattern, markdown_content, re.DOTALL)\n",
    "\n",
    "    return questions\n",
    "\n",
    "questions = parse_questions(markdown_content)\n",
    "\n",
    "questions[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrap each Question with Dspy.Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy \n",
    "\n",
    "trainset = questions[:20]\n",
    "devset = questions[20:30]\n",
    "testset = questions[30:]\n",
    "\n",
    "\n",
    "trainset = [dspy.Example(question=question).with_inputs(\"question\") for question in trainset]\n",
    "devset = [dspy.Example(question=question).with_inputs(\"question\") for question in devset]\n",
    "trainset = [dspy.Example(question=question).with_inputs(\"question\") for question in testset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Example({'question': 'Is there support to multiple versions of the query/document embedding models to co-exist at a given time? (helps with live experiments of new model versions)'}) (input_keys={'question'}),\n",
       " Example({'question': 'How can I retrieve the total object count in a class?'}) (input_keys={'question'}),\n",
       " Example({'question': \"How do I get the cosine similarity from Weaviate's certainty?\"}) (input_keys={'question'}),\n",
       " Example({'question': 'The quality of my search results change depending on the specified limit. Why? How can I fix this?'}) (input_keys={'question'}),\n",
       " Example({'question': 'Why did you use GraphQL instead of SPARQL?'}) (input_keys={'question'}),\n",
       " Example({'question': 'What is the best way to iterate through objects? Can I do paginated API calls?'}) (input_keys={'question'}),\n",
       " Example({'question': 'What is best practice for updating data?'}) (input_keys={'question'}),\n",
       " Example({'question': 'Can I connect my own module?'}) (input_keys={'question'}),\n",
       " Example({'question': 'Can I train my own text2vec-contextionary vectorizer module?'}) (input_keys={'question'}),\n",
       " Example({'question': 'Does Weaviate use Hnswlib?'}) (input_keys={'question'})]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "devset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. LLM Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "metricLM = dspy.OpenAI(model='gpt-4', max_tokens=1000, model_type='chat')\n",
    "\n",
    "class Assess(dspy.Signature):\n",
    "    \"\"\"Assess the  quality of an answer to a question\"\"\"\n",
    "\n",
    "    context = dspy.InputField(desc=\"The context for answering the question.\")\n",
    "    assessed_question = dspy.InputField(desc=\"The Evaluation criterion.\")\n",
    "    assessed_answer = dspy.InputField(desc=\"The answer to the question.\")\n",
    "    assessment_answer = dspy.OutputField(desc=\"A rating between 1 and 5. Only output the rating and nothing else.\")\n",
    "\n",
    "\n",
    "def llm_metric(gold, pred, trace=None):\n",
    "    predicted_answer = pred.answer \n",
    "    question = gold.question \n",
    "    \n",
    "    print(f\"Test Question: {question}\")\n",
    "    print(f\"Predicted Answer: {predicted_answer}\")\n",
    "\n",
    "    detail = \"Is the assessed answer detailed?\"\n",
    "    faithful = \"Is the assessed text grounded in the context? Say no if it includes significant facts not in the context.\"\n",
    "    overall = f\"Please rate how well this answer answers the question, `{question}` based on the context.\\n `{predicted_answer}`\"\n",
    "    \n",
    "    with dspy.context(lm=metricLM):\n",
    "        context = dspy.Retrieve(k=5)(question).passages\n",
    "        detail = dspy.ChainOfThought(Assess)(context=\"N/A\", assessed_question=detail, assessed_answer=predicted_answer)\n",
    "        faithful = dspy.ChainOfThought(Assess)(context=context, assessed_question=faithful, assessed_answer=predicted_answer)\n",
    "        overall = dspy.ChainOfThought(Assess)(context=context, assessed_question=overall, assessed_answer=predicted_answer)\n",
    "    \n",
    "    print(f\"Faithful: {faithful.assessment_answer}\")\n",
    "    print(f\"Detail: {detail.assessment_answer}\")\n",
    "    print(f\"Overall: {overall.assessment_answer}\")\n",
    "    \n",
    "    \n",
    "    total = float(detail.assessment_answer) + float(faithful.assessment_answer)*2 + float(overall.assessment_answer)\n",
    "    \n",
    "    return total / 5.0\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Question: What do cross encoders do?\n",
      "Predicted Answer: They re-rank documents.\n"
     ]
    },
    {
     "ename": "UnexpectedStatusCodeError",
     "evalue": "Query was not successful! Unexpected status code: 502, with response body: None.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnexpectedStatusCodeError\u001b[0m                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m test_example \u001b[38;5;241m=\u001b[39m dspy\u001b[38;5;241m.\u001b[39mExample(question\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat do cross encoders do?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m test_pred \u001b[38;5;241m=\u001b[39m dspy\u001b[38;5;241m.\u001b[39mExample(answer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThey re-rank documents.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28mtype\u001b[39m(llm_metric(test_example, test_pred))\n",
      "Cell \u001b[0;32mIn[8], line 24\u001b[0m, in \u001b[0;36mllm_metric\u001b[0;34m(gold, pred, trace)\u001b[0m\n\u001b[1;32m     21\u001b[0m overall \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease rate how well this answer answers the question, `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquestion\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` based on the context.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpredicted_answer\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m dspy\u001b[38;5;241m.\u001b[39mcontext(lm\u001b[38;5;241m=\u001b[39mmetricLM):\n\u001b[0;32m---> 24\u001b[0m     context \u001b[38;5;241m=\u001b[39m dspy\u001b[38;5;241m.\u001b[39mRetrieve(k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)(question)\u001b[38;5;241m.\u001b[39mpassages\n\u001b[1;32m     25\u001b[0m     detail \u001b[38;5;241m=\u001b[39m dspy\u001b[38;5;241m.\u001b[39mChainOfThought(Assess)(context\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mN/A\u001b[39m\u001b[38;5;124m\"\u001b[39m, assessed_question\u001b[38;5;241m=\u001b[39mdetail, assessed_answer\u001b[38;5;241m=\u001b[39mpredicted_answer)\n\u001b[1;32m     26\u001b[0m     faithful \u001b[38;5;241m=\u001b[39m dspy\u001b[38;5;241m.\u001b[39mChainOfThought(Assess)(context\u001b[38;5;241m=\u001b[39mcontext, assessed_question\u001b[38;5;241m=\u001b[39mfaithful, assessed_answer\u001b[38;5;241m=\u001b[39mpredicted_answer)\n",
      "File \u001b[0;32m~/miniconda3/envs/llms/lib/python3.11/site-packages/dspy/retrieve/retrieve.py:29\u001b[0m, in \u001b[0;36mRetrieve.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 29\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/llms/lib/python3.11/site-packages/dspy/retrieve/retrieve.py:39\u001b[0m, in \u001b[0;36mRetrieve.forward\u001b[0;34m(self, query_or_queries)\u001b[0m\n\u001b[1;32m     33\u001b[0m queries \u001b[38;5;241m=\u001b[39m [query\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m query \u001b[38;5;129;01min\u001b[39;00m queries]\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# print(queries)\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# TODO: Consider removing any quote-like markers that surround the query too.\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m passages \u001b[38;5;241m=\u001b[39m dsp\u001b[38;5;241m.\u001b[39mretrieveEnsemble(queries, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Prediction(passages\u001b[38;5;241m=\u001b[39mpassages)\n",
      "File \u001b[0;32m~/miniconda3/envs/llms/lib/python3.11/site-packages/dsp/primitives/search.py:50\u001b[0m, in \u001b[0;36mretrieveEnsemble\u001b[0;34m(queries, k, by_prob)\u001b[0m\n\u001b[1;32m     47\u001b[0m queries \u001b[38;5;241m=\u001b[39m [q \u001b[38;5;28;01mfor\u001b[39;00m q \u001b[38;5;129;01min\u001b[39;00m queries \u001b[38;5;28;01mif\u001b[39;00m q]\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(queries) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 50\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retrieve(queries[\u001b[38;5;241m0\u001b[39m], k)\n\u001b[1;32m     52\u001b[0m passages \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m q \u001b[38;5;129;01min\u001b[39;00m queries:\n",
      "File \u001b[0;32m~/miniconda3/envs/llms/lib/python3.11/site-packages/dsp/primitives/search.py:9\u001b[0m, in \u001b[0;36mretrieve\u001b[0;34m(query, k, **kwargs)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m dsp\u001b[38;5;241m.\u001b[39msettings\u001b[38;5;241m.\u001b[39mrm:\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo RM is loaded.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m passages \u001b[38;5;241m=\u001b[39m dsp\u001b[38;5;241m.\u001b[39msettings\u001b[38;5;241m.\u001b[39mrm(query, k\u001b[38;5;241m=\u001b[39mk, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     10\u001b[0m passages \u001b[38;5;241m=\u001b[39m [psg\u001b[38;5;241m.\u001b[39mlong_text \u001b[38;5;28;01mfor\u001b[39;00m psg \u001b[38;5;129;01min\u001b[39;00m passages]\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dsp\u001b[38;5;241m.\u001b[39msettings\u001b[38;5;241m.\u001b[39mreranker:\n",
      "File \u001b[0;32m~/miniconda3/envs/llms/lib/python3.11/site-packages/dspy/retrieve/retrieve.py:29\u001b[0m, in \u001b[0;36mRetrieve.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 29\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/llms/lib/python3.11/site-packages/dspy/retrieve/weaviate_rm.py:77\u001b[0m, in \u001b[0;36mWeaviateRM.forward\u001b[0;34m(self, query_or_queries, k)\u001b[0m\n\u001b[1;32m     71\u001b[0m passages \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m query \u001b[38;5;129;01min\u001b[39;00m queries:\n\u001b[1;32m     73\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_weaviate_client\u001b[38;5;241m.\u001b[39mquery\\\n\u001b[1;32m     74\u001b[0m         \u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_weaviate_collection_name, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m])\\\n\u001b[1;32m     75\u001b[0m         \u001b[38;5;241m.\u001b[39mwith_hybrid(query\u001b[38;5;241m=\u001b[39mquery)\\\n\u001b[1;32m     76\u001b[0m         \u001b[38;5;241m.\u001b[39mwith_limit(k)\\\n\u001b[0;32m---> 77\u001b[0m         \u001b[38;5;241m.\u001b[39mdo()\n\u001b[1;32m     79\u001b[0m     results \u001b[38;5;241m=\u001b[39m results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGet\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_weaviate_collection_name]\n\u001b[1;32m     80\u001b[0m     parsed_results \u001b[38;5;241m=\u001b[39m [result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m results]\n",
      "File \u001b[0;32m~/miniconda3/envs/llms/lib/python3.11/site-packages/weaviate/gql/get.py:1913\u001b[0m, in \u001b[0;36mGetBuilder.do\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1911\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m   1912\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1913\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mdo()\n",
      "File \u001b[0;32m~/miniconda3/envs/llms/lib/python3.11/site-packages/weaviate/gql/filter.py:127\u001b[0m, in \u001b[0;36mGraphQL.do\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RequestsConnectionError \u001b[38;5;28;01mas\u001b[39;00m conn_err:\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RequestsConnectionError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQuery was not successful.\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mconn_err\u001b[39;00m\n\u001b[0;32m--> 127\u001b[0m res \u001b[38;5;241m=\u001b[39m _decode_json_response_dict(response, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQuery was not successful\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[0;32m~/miniconda3/envs/llms/lib/python3.11/site-packages/weaviate/util.py:929\u001b[0m, in \u001b[0;36m_decode_json_response_dict\u001b[0;34m(response, location)\u001b[0m\n\u001b[1;32m    926\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m JSONDecodeError:\n\u001b[1;32m    927\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ResponseCannotBeDecodedError(location, response)\n\u001b[0;32m--> 929\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m UnexpectedStatusCodeError(location, response)\n",
      "\u001b[0;31mUnexpectedStatusCodeError\u001b[0m: Query was not successful! Unexpected status code: 502, with response body: None."
     ]
    }
   ],
   "source": [
    "test_example = dspy.Example(question=\"What do cross encoders do?\")\n",
    "test_pred = dspy.Example(answer=\"They re-rank documents.\")\n",
    "\n",
    "type(llm_metric(test_example, test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "metricLM.inspect_history(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. The DSPy Programming MOdel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerateAnswer(dspy.Signature):\n",
    "    \"\"\"Answer questions based on the context.\"\"\"\n",
    "\n",
    "    context = dspy.InputField(desc=\"may contain relevant facts\")\n",
    "    question = dspy.InputField()\n",
    "    answer = dspy.OutputField()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAG(dspy.Module):\n",
    "    def __init__(self, num_passages=3):\n",
    "        super().__init__()\n",
    "\n",
    "        self.retrieve = dspy.Retrieve(k=num_passages)\n",
    "        self.generate_answer = dspy.ChainOfThought(GenerateAnswer)\n",
    "    \n",
    "    def forward(self, question):\n",
    "        context = self.retrieve(question).passages\n",
    "        prediction = self.generate_answer(context=context, question=question)\n",
    "        return dspy.Prediction(answer=prediction.answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "Answer questions based on the context.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Context: may contain relevant facts\n",
      "Question: ${question}\n",
      "Answer: ${answer}\n",
      "\n",
      "---\n",
      "\n",
      "Question: What are Cross-Encoders?\n",
      "Answer:\u001b[32m Context: Cross-Encoders are a type of neural network architecture commonly used in natural language processing tasks.\n",
      "\n",
      "Question: What are Cross-Encoders?\n",
      "Answer: Cross-Encoders are neural networks that process both input sequences simultaneously to generate a single output, allowing them to capture complex interactions between the input sequences.\u001b[0m\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dspy.Predict(GenerateAnswer)(question=\"What are Cross-Encoders?\")\n",
    "llm.inspect_history(n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "Answer questions based on the context.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Context: may contain relevant facts\n",
      "\n",
      "Question: ${question}\n",
      "\n",
      "Reasoning: Let's think step by step in order to ${produce the answer}. We ...\n",
      "\n",
      "Answer: ${answer}\n",
      "\n",
      "---\n",
      "\n",
      "Context:\n",
      "produce the answer. We know that Cross Encoders are a type of neural network architecture commonly used in natural language processing tasks. They are designed to take two input sequences and produce a single output, typically used for tasks like sentence similarity or paraphrase detection.\n",
      "\n",
      "Answer: Cross Encoders are a type of neural network architecture used in natural language processing tasks to process two input sequences and produce a single output.\n",
      "\n",
      "Question: What are Cross Encoders?\n",
      "\n",
      "Reasoning: Let's think step by step in order to\u001b[32m produce the answer. We know that Cross Encoders are a type of neural network architecture commonly used in natural language processing tasks. They are designed to take two input sequences and produce a single output, typically used for tasks like sentence similarity or paraphrase detection.\n",
      "\n",
      "Answer: Cross Encoders are a type of neural network architecture used in natural language processing tasks to process two input sequences and\u001b[0m\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dspy.ChainOfThought(GenerateAnswer)(question=\"What are Cross Encoders?\")\n",
    "llm.inspect_history(n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "You will be given `context`, `question` and you will respond with `answer`.\n",
      "\n",
      "To do this, you will interleave Thought, Action, and Observation steps.\n",
      "\n",
      "Thought can reason about the current situation, and Action can be the following types:\n",
      "\n",
      "(1) Search[query], which takes a search query and returns one or more potentially relevant passages from a corpus\n",
      "(2) Finish[answer], which returns the final `answer` and finishes the task\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Context: may contain relevant facts\n",
      "\n",
      "Question: ${question}\n",
      "\n",
      "Thought 1: next steps to take based on last observation\n",
      "\n",
      "Action 1: always either Search[query] or, when done, Finish[answer]\n",
      "\n",
      "Observation 1: observations based on action\n",
      "\n",
      "Thought 2: next steps to take based on last observation\n",
      "\n",
      "Action 2: always either Search[query] or, when done, Finish[answer]\n",
      "\n",
      "Observation 2: observations based on action\n",
      "\n",
      "Thought 3: next steps to take based on last observation\n",
      "\n",
      "Action 3: always either Search[query] or, when done, Finish[answer]\n",
      "\n",
      "Observation 3: observations based on action\n",
      "\n",
      "Thought 4: next steps to take based on last observation\n",
      "\n",
      "Action 4: always either Search[query] or, when done, Finish[answer]\n",
      "\n",
      "---\n",
      "\n",
      "Context:\n",
      "I need to find information about what cross encoders are.\n",
      "\n",
      "Action 1: Search[cross encoders]\n",
      "\n",
      "Question: What are cross encoders?\n",
      "\n",
      "Thought 1: \n",
      "\n",
      "Action 1: I need to understand the concept of cross encoders to provide an accurate answer. Action 1: Search[cross encoders]\n",
      "\n",
      "Observation 1: Failed to parse action. Bad formatting or incorrect action name.\n",
      "\n",
      "Thought 2: I need to search for information on cross encoders to understand the concept better.\n",
      "\n",
      "Action 2: Search[what are cross encoders]\n",
      "\n",
      "Observation 2: Failed to parse action. Bad formatting or incorrect action name.\n",
      "\n",
      "Thought 3: I need to search for information on cross encoders to understand the concept better.\n",
      "\n",
      "Action 3: Search[Cross encoders definition]\n",
      "\n",
      "Observation 3: Failed to parse action. Bad formatting or incorrect action name.\n",
      "\n",
      "Thought 4:\u001b[32m I need to search for information on cross encoders to understand the concept better.\n",
      "\n",
      "Action 4: Search[definition of cross encoders]\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "You will be given `context`, `question` and you will respond with `answer`.\n",
      "\n",
      "To do this, you will interleave Thought, Action, and Observation steps.\n",
      "\n",
      "Thought can reason about the current situation, and Action can be the following types:\n",
      "\n",
      "(1) Search[query], which takes a search query and returns one or more potentially relevant passages from a corpus\n",
      "(2) Finish[answer], which returns the final `answer` and finishes the task\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Context: may contain relevant facts\n",
      "\n",
      "Question: ${question}\n",
      "\n",
      "Thought 1: next steps to take based on last observation\n",
      "\n",
      "Action 1: always either Search[query] or, when done, Finish[answer]\n",
      "\n",
      "Observation 1: observations based on action\n",
      "\n",
      "Thought 2: next steps to take based on last observation\n",
      "\n",
      "Action 2: always either Search[query] or, when done, Finish[answer]\n",
      "\n",
      "Observation 2: observations based on action\n",
      "\n",
      "Thought 3: next steps to take based on last observation\n",
      "\n",
      "Action 3: always either Search[query] or, when done, Finish[answer]\n",
      "\n",
      "Observation 3: observations based on action\n",
      "\n",
      "Thought 4: next steps to take based on last observation\n",
      "\n",
      "Action 4: always either Search[query] or, when done, Finish[answer]\n",
      "\n",
      "Observation 4: observations based on action\n",
      "\n",
      "Thought 5: next steps to take based on last observation\n",
      "\n",
      "Action 5: always either Search[query] or, when done, Finish[answer]\n",
      "\n",
      "---\n",
      "\n",
      "Context:\n",
      "I need to find information about what cross encoders are.\n",
      "\n",
      "Action 1: Search[cross encoders]\n",
      "\n",
      "Question: What are cross encoders?\n",
      "\n",
      "Thought 1: \n",
      "\n",
      "Action 1: I need to understand the concept of cross encoders to provide an accurate answer. Action 1: Search[cross encoders]\n",
      "\n",
      "Observation 1: Failed to parse action. Bad formatting or incorrect action name.\n",
      "\n",
      "Thought 2: I need to search for information on cross encoders to understand the concept better.\n",
      "\n",
      "Action 2: Search[what are cross encoders]\n",
      "\n",
      "Observation 2: Failed to parse action. Bad formatting or incorrect action name.\n",
      "\n",
      "Thought 3: I need to search for information on cross encoders to understand the concept better.\n",
      "\n",
      "Action 3: Search[Cross encoders definition]\n",
      "\n",
      "Observation 3: Failed to parse action. Bad formatting or incorrect action name.\n",
      "\n",
      "Thought 4: I need to search for information on cross encoders to understand the concept better.\n",
      "\n",
      "Action 4: Search[definition of cross encoders]\n",
      "\n",
      "Observation 4: Failed to parse action. Bad formatting or incorrect action name.\n",
      "\n",
      "Thought 5:\u001b[32m I need to search for information on cross encoders to understand the concept better.\n",
      "\n",
      "Action 5: Search[explanation of cross encoders]\u001b[0m\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dspy.ReAct(GenerateAnswer, tools=[dspy.settings.rm])(question=\"What are cross encoders?\")\n",
    "llm.inspect_history(n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intializa DSPy Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "uncompiled_rag = RAG()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-rankers in search engines are mechanisms that refine the relevance of search results by using different approaches such as content-based re-ranking (Cross Encoders) or context-based re-ranking using symbolic features (Metadata Rankers).\n"
     ]
    }
   ],
   "source": [
    "print(uncompiled_rag(\"What are re-rankers in search engines?\").answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "Answer questions based on the context.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Context: may contain relevant facts\n",
      "\n",
      "Question: ${question}\n",
      "\n",
      "Reasoning: Let's think step by step in order to ${produce the answer}. We ...\n",
      "\n",
      "Answer: ${answer}\n",
      "\n",
      "---\n",
      "\n",
      "Context:\n",
      "[1] «They offer the advantage of further reasoning about the relevance of results without needing specialized training. Cross Encoders can be interfaced with Weaviate to re-rank search results, trading off performance for slower search speed. * **Metadata Rankers** are context-based re-rankers that use symbolic features to rank relevance. They take into account user and document features, such as age, gender, location, preferences, release year, genre, and box office, to predict the relevance of candidate documents. By incorporating metadata features, these rankers offer a more personalized and context-aware search experience.»\n",
      "[2] «However, most of the LLM APIs don’t actually give us these probabilities. Further, this is probably pretty slow. We will keep an eye on it, but it doesn’t seem like the next step to take for now. ## Metadata Rankers\n",
      "Whereas I would describe Cross-Encoders as `content-based` re-ranking, I would say Metadata rankers are `context-based` re-rankers. Metadata rankers describe using symbolic features to rank relevance.»\n",
      "[3] «Taken directly from the paper, “Our findings indicate that cross-encoder re-rankers can efficiently be improved without additional computational burden and extra steps in the pipeline by explicitly adding the output of the first-stage ranker to the model input, and this effect is robust for different models and query types”. Taking this a bit further, [Dinh et al.](https://arxiv.org/abs/2206.06565) shows that most tabular machine learning tasks can be translated to text and benefit from transfer learning of text-based models. Many of these metadata rankers may also take in something like a collaborative filtering score that is based on this user’s history, as well as other users on the platform — another interesting feature to think of interfacing this way. The main point being, maybe we can just add these meta features to our [query, document] representation and keep the Zero-Shot party going. We recently had an interesting discussion about metadata ranking and future directions for ranking models broadly on our latest Weaviate podcast! 👉 Check it out [here](https://www.youtube.com/watch?v=aLY0q6V01G4)\n",
      "\n",
      "## Score Rankers\n",
      "Score rankers describe using either a classifier to detect things, or a regression model to score things, about our candidate documents to rank with.»\n",
      "\n",
      "Question: What are re-rankers in search engines?\n",
      "\n",
      "Reasoning: Let's think step by step in order to\u001b[32m produce the answer. We can see from the context that re-rankers in search engines are mechanisms used to further refine the relevance of search results. Cross Encoders and Metadata Rankers are two types of re-rankers mentioned in the text. Cross Encoders focus on content-based re-ranking, while Metadata Rankers are context-based re-rankers that use symbolic features to predict relevance based on user and document features.\n",
      "\n",
      "Answer: Re-rankers in search engines are mechanisms that refine the relevance of search results by using different approaches such as content-based re-ranking (Cross Encoders) or context-based re-ranking using symbolic features (Metadata Rankers).\u001b[0m\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "llm.inspect_history(n=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. DSPy Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Example({'question': 'Is there support to multiple versions of the query/document embedding models to co-exist at a given time? (helps with live experiments of new model versions)'}) (input_keys={'question'})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "devset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for example in dev set: \t\t Query was not successful! Unexpected status code: 502, with response body: None.\n",
      "Test Question: How can I retrieve the total object count in a class?\n",
      "Predicted Answer: To retrieve the total object count in a class in Weaviate, you can use the endpoint `/v1/objects/{ClassName}` where `{ClassName}` is the name of the class you want to retrieve the object count for.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 1\n",
      "Detail: 5\n",
      "Overall: 3\n",
      "Test Question: How do I get the cosine similarity from Weaviate's certainty?\n",
      "Predicted Answer: To get the cosine similarity from Weaviate's certainty, you can directly use the certainty value as an approximation of the cosine similarity.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 1\n",
      "Detail: 3\n",
      "Overall: 5\n",
      "Test Question: The quality of my search results change depending on the specified limit. Why? How can I fix this?\n",
      "Predicted Answer: The quality of search results can change depending on where relevant information is placed in the search results. To potentially fix this, experiment with different placements of relevant information in the search results to see how it impacts the quality\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 5\n",
      "Detail: 5\n",
      "Overall: 2\n",
      "Test Question: Why did you use GraphQL instead of SPARQL?\n",
      "Predicted Answer: The decision to use GraphQL instead of SPARQL was likely influenced by the fact that GraphQL allows for more efficient traversal of the hierarchical representation used in HNSW. SPARQL, being a query language for RDF databases, may not be as well-suited for this specific hierarchical structure. Additionally, GraphQL's flexibility and ability to handle complex nested queries may have made it a more suitable choice for navigating through the layers of the graph in HNSW.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 1\n",
      "Detail: 5\n",
      "Overall: 2\n",
      "Test Question: What is the best way to iterate through objects? Can I do paginated API calls?\n",
      "Predicted Answer: The best way to iterate through objects in Weaviate is to use paginated API calls. This allows you to retrieve a limited number of objects at a time, reducing memory allocation and improving performance. By implementing pagination, you can efficiently process objects without overwhelming the system with unnecessary memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 1\n",
      "Detail: 5\n",
      "Overall: 1\n",
      "Test Question: What is best practice for updating data?\n",
      "Predicted Answer: The best practice for updating data includes adding a deduplication process, running the recovery process in parallel, and implementing a mechanism to flush idle memtables after 60 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 5\n",
      "Detail: 5\n",
      "Overall: 5\n",
      "Test Question: Can I connect my own module?\n",
      "Predicted Answer: No, the context mentions that Weaviate coordinates efforts around data imports, updates, queries, etc., and delegates requests to specific modules like the Hugging Face module. It also states that every new Weaviate instance created with Weaviate Cloud Services has the Hugging Face module enabled by default. Therefore, it does not seem like users can connect their own modules to Weaviate.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 1\n",
      "Detail: 5\n",
      "Overall: 3\n",
      "Test Question: Can I train my own text2vec-contextionary vectorizer module?\n",
      "Predicted Answer: No, the Weaviate platform does not currently support training custom text2vec-contextionary vectorizer modules. However, it does support using custom transformer models that are compatible with Hugging Face's `AutoModel` and `AutoTokenizer`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 2\n",
      "Detail: 5\n",
      "Overall: 5\n",
      "Test Question: Does Weaviate use Hnswlib?\n",
      "Predicted Answer: No, Weaviate does not use Hnswlib.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 21.6 / 10  (216.0): 100%|██████████| 10/10 [04:02<00:00, 24.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful: 1\n",
      "Detail: 3\n",
      "Overall: 5\n",
      "Average Metric: 21.6 / 10  (216.0%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/Users/anshulsingh/miniconda3/envs/llms/lib/python3.11/site-packages/dspy/evaluate/evaluate.py:137: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(truncate_cell)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_0bc91 th {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_0bc91 td {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_0bc91_row0_col0, #T_0bc91_row0_col1, #T_0bc91_row0_col2, #T_0bc91_row1_col0, #T_0bc91_row1_col1, #T_0bc91_row1_col2, #T_0bc91_row2_col0, #T_0bc91_row2_col1, #T_0bc91_row2_col2, #T_0bc91_row3_col0, #T_0bc91_row3_col1, #T_0bc91_row3_col2, #T_0bc91_row4_col0, #T_0bc91_row4_col1, #T_0bc91_row4_col2 {\n",
       "  text-align: left;\n",
       "  white-space: pre-wrap;\n",
       "  word-wrap: break-word;\n",
       "  max-width: 400px;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_0bc91\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_0bc91_level0_col0\" class=\"col_heading level0 col0\" >question</th>\n",
       "      <th id=\"T_0bc91_level0_col1\" class=\"col_heading level0 col1\" >llm_metric</th>\n",
       "      <th id=\"T_0bc91_level0_col2\" class=\"col_heading level0 col2\" >answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_0bc91_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_0bc91_row0_col0\" class=\"data row0 col0\" >Is there support to multiple versions of the query/document embedding models to co-exist at a given time? (helps with live experiments of new model versions)</td>\n",
       "      <td id=\"T_0bc91_row0_col1\" class=\"data row0 col1\" >0.0</td>\n",
       "      <td id=\"T_0bc91_row0_col2\" class=\"data row0 col2\" >nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0bc91_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_0bc91_row1_col0\" class=\"data row1 col0\" >How can I retrieve the total object count in a class?</td>\n",
       "      <td id=\"T_0bc91_row1_col1\" class=\"data row1 col1\" >2.0</td>\n",
       "      <td id=\"T_0bc91_row1_col2\" class=\"data row1 col2\" >To retrieve the total object count in a class in Weaviate, you can use the endpoint `/v1/objects/{ClassName}` where `{ClassName}` is the name of the class...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0bc91_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_0bc91_row2_col0\" class=\"data row2 col0\" >How do I get the cosine similarity from Weaviate's certainty?</td>\n",
       "      <td id=\"T_0bc91_row2_col1\" class=\"data row2 col1\" >2.0</td>\n",
       "      <td id=\"T_0bc91_row2_col2\" class=\"data row2 col2\" >To get the cosine similarity from Weaviate's certainty, you can directly use the certainty value as an approximation of the cosine similarity.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0bc91_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_0bc91_row3_col0\" class=\"data row3 col0\" >The quality of my search results change depending on the specified limit. Why? How can I fix this?</td>\n",
       "      <td id=\"T_0bc91_row3_col1\" class=\"data row3 col1\" >3.4</td>\n",
       "      <td id=\"T_0bc91_row3_col2\" class=\"data row3 col2\" >The quality of search results can change depending on where relevant information is placed in the search results. To potentially fix this, experiment with different...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0bc91_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_0bc91_row4_col0\" class=\"data row4 col0\" >Why did you use GraphQL instead of SPARQL?</td>\n",
       "      <td id=\"T_0bc91_row4_col1\" class=\"data row4 col1\" >1.8</td>\n",
       "      <td id=\"T_0bc91_row4_col2\" class=\"data row4 col2\" >The decision to use GraphQL instead of SPARQL was likely influenced by the fact that GraphQL allows for more efficient traversal of the hierarchical representation...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x16cd983d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                <div style='\n",
       "                    text-align: center; \n",
       "                    font-size: 16px; \n",
       "                    font-weight: bold; \n",
       "                    color: #555; \n",
       "                    margin: 10px 0;'>\n",
       "                    ... 5 more rows not displayed ...\n",
       "                </div>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "216.0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dspy.evaluate.evaluate import Evaluate\n",
    "\n",
    "evaluate = Evaluate(devset=devset, num_threads=1, display_progress=True, display_table=5)\n",
    "\n",
    "evaluate(RAG(), metric=llm_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
